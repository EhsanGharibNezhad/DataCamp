{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Configuration Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Files Location**\n",
    "\n",
    "* Most data files for the exercises can be found on the [course site](https://www.datacamp.com/courses/merging-dataframes-with-pandas)\n",
    "    * [Baby Names](https://assets.datacamp.com/production/repositories/516/datasets/43c9b6bf4c283ab024b2d7d61fbf15a0baa1e44d/Baby%20names.zip)\n",
    "    * [Summer Olympic Medals](https://assets.datacamp.com/production/repositories/516/datasets/2d14df8d3c6a1773358fa000f203282c2e1107d6/Summer%20Olympic%20medals.zip)\n",
    "    * [Automobile Fuel Efficiency](https://assets.datacamp.com/production/repositories/516/datasets/2f3d8b2156d5669fb7e12137f1c2e979c3c9ce0b/automobiles.csv)\n",
    "    * [Exchange Rates](https://assets.datacamp.com/production/repositories/516/datasets/e91482db6a7bae394653278e4e908e63ed9ac833/exchange.csv)\n",
    "    * [GDP](https://assets.datacamp.com/production/repositories/516/datasets/a0858a700501f88721ca9e4bdfca99b9e10b937f/GDP.zip)\n",
    "    * [Oil Prices](https://assets.datacamp.com/production/repositories/516/datasets/707566cf46c4dd6290b9029f5e07a92baf3fe3f7/oil_price.csv)\n",
    "    * [Pittsburgh Weather](https://assets.datacamp.com/production/repositories/516/datasets/58c1ead59818b2451324e9e84239db7bda6b11d3/pittsburgh2013.csv)\n",
    "    * [Sales](https://assets.datacamp.com/production/repositories/516/datasets/2b89c1b00016e1ebcfd7f08a127d2c79589ce5c0/Sales.zip)\n",
    "    * [S&P 500](https://assets.datacamp.com/production/repositories/516/datasets/7a9b570a02ef589891d9576a86876a616ca5f3c8/sp500.csv)\n",
    "* Other data files may be found in my [DataCamp repository](https://github.com/trenton3983/DataCamp/tree/master/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data File Objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / 'data'\n",
    "auto_fuel_file = data / 'merging-dataframes-with-pandas' / 'auto_fuel_efficiency.csv'\n",
    "baby_1881_file = data / 'merging-dataframes-with-pandas' / 'baby_names1881.csv'\n",
    "baby_1981_file = data / 'merging-dataframes-with-pandas' / 'baby_names1981.csv'\n",
    "exch_rates_file = data / 'merging-dataframes-with-pandas' / 'exchange_rates.csv'\n",
    "gdp_china_file = data / 'merging-dataframes-with-pandas' / 'gdp_china.csv'\n",
    "gdp_usa_file = data / 'merging-dataframes-with-pandas' / 'gdp_usa.csv'\n",
    "oil_price_file = data / 'merging-dataframes-with-pandas' / 'oil_price.csv'\n",
    "pitts_file = data / 'merging-dataframes-with-pandas' / 'pittsburgh_weather_2013.csv'\n",
    "sales_feb_hardware_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Hardware.csv'\n",
    "sales_feb_service_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Service.csv'\n",
    "sales_feb_software_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Software.csv'\n",
    "sales_jan_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-jan-2015.csv'\n",
    "sales_feb_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-2015.csv'\n",
    "sales_mar_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-mar-2015.csv'\n",
    "sp500_file = data / 'merging-dataframes-with-pandas' / 'sp500.csv'\n",
    "so_bronze_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Bronze.csv'\n",
    "so_bronze5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_bronze_top5.csv'\n",
    "so_gold_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Gold.csv'\n",
    "so_gold5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_gold_top5.csv'\n",
    "so_silver_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Silver.csv'\n",
    "so_silver5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_silver_top5.csv'\n",
    "so_all_medalists_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - ALL MEDALISTS.tsv'\n",
    "so_editions_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - EDITIONS.tsv'\n",
    "so_ioc_codes_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - IOC COUNTRY CODES.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Merging DataFrames with pandas\n",
    "\n",
    "***Course Description***\n",
    "\n",
    "As a Data Scientist, you'll often find that the data you need is not in a single file. It may be spread across a number of text files, spreadsheets, or databases. You want to be able to import the data of interest as a collection of DataFrames and figure out how to combine them to answer your central questions. This course is all about the act of combining, or merging, DataFrames, an essential part of any working Data Scientist's toolbox. You'll hone your pandas skills by learning how to organize, reshape, and aggregate multiple data sets to answer your specific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "In this chapter, you'll learn about different techniques you can use to import multiple files into DataFrames. Having imported your data into individual DataFrames, you'll then learn how to share information between DataFrames using their Indexes. Understanding how Indexes work is essential information that you'll need for merging DataFrames later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Reading multiple data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools for pandas data import\n",
    "\n",
    "* pd.read_csv() for CSV files\n",
    "    * dataframe = pd.read_csv(filepath)\n",
    "    * dozens of optional input parameters\n",
    "* Other data import tools:\n",
    "    * pd.read_excel()\n",
    "    * pd.read_html()\n",
    "    * pd.read_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading separate files\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "dataframe0 = pd.read_csv('sales-jan-2015.csv')\n",
    "dataframe1 = pd.read_csv('sales-feb-2015.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a loop\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = []\n",
    "for f in filenames:\n",
    "    dataframes.append(pd.read_csv(f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a comprehension\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using glob\n",
    "\n",
    "```python\n",
    "from glob import glob\n",
    "filenames = glob('sales*.csv')\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining DataFrames from multiple data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Indexes\" vs. \"Indices\"\n",
    "\n",
    "* indices: many index labels within Index data structures\n",
    "* indexes: many pandas Index data structures\n",
    "\n",
    "![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/merging_dataframes_in_python/indices_indexes.JPG \"Indices & Indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "w_mean = pd.read_csv('quarterly_mean_temp.csv', index_col='Month')\n",
    "w_max = pd.read_csv('quarterly_max_temp.csv', index_col='Month')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the data\n",
    "\n",
    "```python\n",
    "print(w_mean)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "\n",
    "print(w_max)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68\n",
    "Apr     89\n",
    "Jul     91\n",
    "Oct     84\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame indexes\n",
    "\n",
    "```python\n",
    "print(w_mean.index)\n",
    "Index(['Apr', 'Jan', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(w_max.index)\n",
    "Index(['Jan', 'Apr', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(type(w_mean.index))\n",
    "<class 'pandas.indexes.base.Index'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reindex()\n",
    "\n",
    "```python\n",
    "ordered = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "w_mean2 = w_mean.reindex(ordered)\n",
    "print(w_mean2)\n",
    "\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .sort_index()\n",
    "\n",
    "```python\n",
    "w_mean2.sort_index()\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing with missing labels\n",
    "\n",
    "```python\n",
    "w_mean3 = w_mean.reindex(['Jan', 'Apr', 'Dec'])\n",
    "print(w_mean3)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Dec     NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean3.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "Dec     NaN\n",
    "\n",
    "w_max.reindex(w_mean3.index).dropna()\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order matters\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Apr     89\n",
    "Jan     68\n",
    "Jul     91\n",
    "Oct     84\n",
    "\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting DataFrame with the Index & columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame using another DataFrame Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic with Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "weather = pd.read_csv('pittsburgh2013.csv', index_col='Date', parse_dates=True)\n",
    "weather.loc['2013-7-1':'2013-7-7', 'PrecipitationIn']\n",
    "\n",
    "Date\n",
    "2013-07-01 0.18\n",
    "2013-07-02 0.14\n",
    "2013-07-03 0.00\n",
    "2013-07-04 0.25\n",
    "2013-07-05 0.02\n",
    "2013-07-06 0.06\n",
    "2013-07-07 0.10\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar multiplication\n",
    "\n",
    "```python\n",
    "weather.loc['2013-07-01':'2013-07-07', 'PrecipitationIn'] * 2.54\n",
    "\n",
    "Date\n",
    "2013-07-01 0.4572\n",
    "2013-07-02 0.3556\n",
    "2013-07-03 0.0000\n",
    "2013-07-04 0.6350\n",
    "2013-07-05 0.0508\n",
    "2013-07-06 0.1524\n",
    "2013-07-07 0.2540\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute temperature range\n",
    "\n",
    "```python\n",
    "week1_range = weather.loc['2013-07-01':'2013-07-07', ['Min TemperatureF', 'Max TemperatureF']]\n",
    "print(week1_range)\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 66 79\n",
    "2013-07-02 66 84\n",
    "2013-07-03 71 86\n",
    "2013-07-04 70 86\n",
    "2013-07-05 69 86\n",
    "2013-07-06 70 89\n",
    "2013-07-07 70 77\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average temperature\n",
    "\n",
    "```python\n",
    "week1_mean = weather.loc['2013-07-01':'2013-07-07', 'Mean TemperatureF']\n",
    "print(week1_mean)\n",
    "Date\n",
    "2013-07-01 72\n",
    "2013-07-02 74\n",
    "2013-07-03 78\n",
    "2013-07-04 77\n",
    "2013-07-05 76\n",
    "2013-07-06 78\n",
    "2013-07-07 72\n",
    "Name: Mean TemperatureF, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range / week1_mean\n",
    "RuntimeWarning: Cannot compare type 'Timestamp' with type 'str', sort order is\n",
    "undefined for incomparable objects\n",
    "return this.join(other, how=how, return_indexers=return_indexers)\n",
    "\n",
    "2013-07-01 00:00:00 2013-07-02 00:00:00 2013-07-03 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "2013-07-02 NaN NaN NaN\n",
    "2013-07-03 NaN NaN NaN\n",
    "2013-07-04 NaN NaN NaN\n",
    "2013-07-05 NaN NaN NaN\n",
    "2013-07-06 NaN NaN NaN\n",
    "2013-07-07 NaN NaN NaN\n",
    "2013-07-04 00:00:00 2013-07-05 00:00:00 2013-07-06 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "... ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range.divide(week1_mean, axis='rows')\n",
    "\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 0.916667 1.097222\n",
    "2013-07-02 0.891892 1.135135\n",
    "2013-07-03 0.910256 1.102564\n",
    "2013-07-04 0.909091 1.116883\n",
    "2013-07-05 0.907895 1.131579\n",
    "2013-07-06 0.897436 1.141026\n",
    "2013-07-07 0.972222 1.069444\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage changes\n",
    "\n",
    "```python\n",
    "week1_mean.pct_change() * 100\n",
    "\n",
    "Date\n",
    "2013-07-01 NaN\n",
    "2013-07-02 2.777778\n",
    "2013-07-03 5.405405\n",
    "2013-07-04 -1.282051\n",
    "2013-07-05 -1.298701\n",
    "2013-07-06 2.631579\n",
    "2013-07-07 -7.692308\n",
    "Name: Mean TemperatureF, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bronze Olympic medals\n",
    "\n",
    "```python\n",
    "bronze = pd.read_csv('bronze_top5.csv', index_col=0)\n",
    "print(bronze)\n",
    "Total\n",
    "Country\n",
    "United States 1052.0\n",
    "Soviet Union 584.0\n",
    "United Kingdom 505.0\n",
    "France 475.0\n",
    "Germany 454.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver Olympic medals\n",
    "\n",
    "```python\n",
    "silver = pd.read_csv('silver_top5.csv', index_col=0)\n",
    "print(silver)\n",
    "Total\n",
    "Country\n",
    "United States 1195.0\n",
    "Soviet Union 627.0\n",
    "United Kingdom 591.0\n",
    "France 461.0\n",
    "Italy 394.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Olympic medals\n",
    "\n",
    "```python\n",
    "gold = pd.read_csv('gold_top5.csv', index_col=0)\n",
    "print(gold)\n",
    "Total\n",
    "Country\n",
    "United States 2088.0\n",
    "Soviet Union 838.0\n",
    "United Kingdom 498.0\n",
    "Italy 460.0\n",
    "Germany 407.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "In [22]: print(bronze['United States'])\n",
    "1052.0\n",
    "In [23]: print(silver['United States'])\n",
    "1195.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the .add() method\n",
    "\n",
    "```python\n",
    "bronze.add(silver)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a fill_value\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 454.0\n",
    "Italy 394.0\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver, gold\n",
    "\n",
    "```python\n",
    "bronze + silver + gold\n",
    "\n",
    "Country\n",
    "France NaN\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining .add()\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0).add(gold, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 861.0\n",
    "Italy 854.0\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding unaligned DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting in Arithmetic formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage growth of GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting currency of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Data\n",
    "\n",
    "Having learned how to import multiple DataFrames and share information using Indexes, in this chapter you'll learn how to perform database-style operations to combine DataFrames. In particular, you'll learn about appending and concatenating DataFrames while working with a variety of real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append()\n",
    "\n",
    "* .append(): Series & DataFrame method\n",
    "* Invocation:\n",
    "* s1.append(s2)\n",
    "* Stacks rows of s2 below s1\n",
    "* Method for Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat()\n",
    "\n",
    "* concat(): pandas module function\n",
    "* Invocation:\n",
    "* pd.concat([s1, s2, s3])\n",
    "* Can stack row-wise or column-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat() & .append()\n",
    "\n",
    "* Equivalence of concat() & .append():\n",
    "* result1 = pd.concat([s1, s2, s3])\n",
    "* result2 = s1.append(s2).append(s3)\n",
    "* result1 == result2 elementwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series of US states\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "northeast = pd.Series(['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'])\n",
    "south = pd.Series(['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'])\n",
    "midwest = pd.Series(['IL', 'IN', 'MN', 'MO', 'NE', 'ND', 'SD', 'IA', 'KS', 'MI', 'OH', 'WI'])\n",
    "west = pd.Series(['AZ', 'CO', 'ID', 'MT',\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .append()\n",
    "\n",
    "```python\n",
    "east = northeast.append(south)\n",
    "print(east)\n",
    "0 CT       7 DC\n",
    "1 ME       8 WV\n",
    "2 MA       9 AL\n",
    "3 NH       10 KY\n",
    "4 RI       11 MS\n",
    "5 VT       12 TN\n",
    "6 NJ       13 AR\n",
    "7 NY       14 LA\n",
    "8 PA       15 OK\n",
    "0 DE       16 TX\n",
    "1 FL       dtype: object\n",
    "2 GA\n",
    "3 MD\n",
    "4 NC\n",
    "5 SC\n",
    "6 VA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The appended Index\n",
    "\n",
    "```python\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "\n",
    "print(east.loc[3])\n",
    "3 NH\n",
    "3 MD\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reset_index()\n",
    "\n",
    "```python\n",
    "new_east = northeast.append(south).reset_index(drop=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using concat()\n",
    "\n",
    "```python\n",
    "east = pd.concat([northeast, south])\n",
    "print(east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "0 DE\n",
    "1 FL\n",
    "dtype: object\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ignore_index\n",
    "\n",
    "```python\n",
    "new_east = pd.concat([northeast, south], ignore_index=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending Series with nonunique Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading population data\n",
    "\n",
    "```python\n",
    "In [1]: import pandas as pd\n",
    "In [2]: pop1 = pd.read_csv('population_01.csv', index_col=0)\n",
    "In [3]: pop2 = pd.read_csv('population_02.csv', index_col=0)\n",
    "In [4]: print(type(pop1), pop1.shape)\n",
    "<class 'pandas.core.frame.DataFrame'> (4, 1)\n",
    "In [5]: print(type(pop2), pop2.shape)\n",
    "<class 'pandas.core.frame.DataFrame'> (4, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining population data\n",
    "\n",
    "```python\n",
    "print(pop1)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "66407 479\n",
    "72732 4716\n",
    "50579 2405\n",
    "46241 30670\n",
    "print(pop2)\n",
    "\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "12776 2180\n",
    "76092 26669\n",
    "98360 12221\n",
    "49464 27481\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population DataFrames\n",
    "\n",
    "```python\n",
    "pop1.append(pop2)\n",
    "\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "66407 479\n",
    "72732 4716\n",
    "50579 2405\n",
    "46241 30670\n",
    "12776 2180\n",
    "76092 26669\n",
    "98360 12221\n",
    "49464 27481\n",
    "\n",
    "print(pop1.index.name, pop1.columns)\n",
    "Zip Code ZCTA Index(['2010 Census Population'], dtype='object')\n",
    "\n",
    "print(pop2.index.name, pop2.columns)\n",
    "Zip Code ZCTA Index(['2010 Census Population'], dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "print(population)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538 322\n",
    "59916 130\n",
    "37660 40038\n",
    "2860 45199\n",
    "\n",
    "print(unemployment)\n",
    "unemployment participants\n",
    "Zip\n",
    "2860 0.11 34447\n",
    "46167 0.02 4800\n",
    "1097 0.33 42\n",
    "80808 0.07 4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population & unemployment\n",
    "\n",
    "```python\n",
    "population.append(unemployment)\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated index labels\n",
    "\n",
    "```python\n",
    "population.append(unemployment)\n",
    "\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=0)\n",
    "\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating columns\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1)\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "1097 NaN 0.33 42.0\n",
    "2860 45199.0 0.11 34447.0\n",
    "37660 40038.0 NaN NaN\n",
    "46167 NaN 0.02 4800.0\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "80808 NaN 0.07 4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending DataFrames with ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas DataFrames along column axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation, keys & MultiIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading rainfall data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "file1 = 'q1_rainfall_2013.csv'\n",
    "rain2013 = pd.read_csv(file1, index_col='Month', parse_dates=True)\n",
    "file2 = 'q1_rainfall_2014.csv'\n",
    "rain2014 = pd.read_csv(file2, index_col='Month', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining rainfall data\n",
    "\n",
    "```python\n",
    "print(rain2013)\n",
    "Precipitation\n",
    "Month\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "\n",
    "print(rain2014)\n",
    "Precipitation\n",
    "Month\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows\n",
    "\n",
    "```python\n",
    "pd.concat([rain2013, rain2014], axis=0)\n",
    "\n",
    "Precipitation\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using multi-index on rows\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis=0)\n",
    "print(rain1314)\n",
    "Precipitation\n",
    "2013 Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "2014 Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a multi-index\n",
    "\n",
    "```python\n",
    "print(rain1314.loc[2014])\n",
    "Precipitation\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating columns\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], axis='columns')\n",
    "print(rain1314)\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a multi-index on columns\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis='columns')\n",
    "print(rain1314)\n",
    "2013 2014\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "\n",
    "print(rain1314[2013])\n",
    "Precipitation\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat() with dict\n",
    "\n",
    "```python\n",
    "rain_dict = {2013: rain2013, 2014: rain2014}\n",
    "rain1314 = pd.concat(rain_dict, axis='columns')\n",
    "print(rain1314)\n",
    "2013 2014\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating vertically to get MultiIndexed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing MultiIndexed DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating horizontally to get MultiIndexed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames from a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer & inner joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using with arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(8).reshape(2, 4) + 0.1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.arange(6).reshape(2,3) + 0.2\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(12).reshape(3,4) + 0.3\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([B, A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([B, A], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([A, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incompatible array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, B], axis=0) # incompatible columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=1) # incompatible rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "print(population)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538 322\n",
    "59916 130\n",
    "37660 40038\n",
    "2860 45199\n",
    "\n",
    "print(unemployment)\n",
    "unemployment participants\n",
    "Zip\n",
    "2860 0.11 34447\n",
    "46167 0.02 4800\n",
    "1097 0.33 42\n",
    "80808 0.07 4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to arrays\n",
    "\n",
    "```python\n",
    "population_array = np.array(population)\n",
    "print(population_array) # Index info is lost\n",
    "[[ 322]\n",
    "[ 130]\n",
    "[40038]\n",
    "[45199]]\n",
    "\n",
    "unemployment_array = np.array(unemployment)\n",
    "print(population_array)\n",
    "[[ 1.10000000e-01 3.44470000e+04]\n",
    "[ 2.00000000e-02 4.80000000e+03]\n",
    "[ 3.30000000e-01 4.20000000e+01]\n",
    "[ 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating data as arrays\n",
    "\n",
    "```python\n",
    "print(np.concatenate([population_array, unemployment_array], axis=1))\n",
    "[[ 3.22000000e+02 1.10000000e-01 3.44470000e+04]\n",
    "[ 1.30000000e+02 2.00000000e-02 4.80000000e+03]\n",
    "[ 4.00380000e+04 3.30000000e-01 4.20000000e+01]\n",
    "[ 4.51990000e+04 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "\n",
    "* Joining tables: Combining rows of multiple tables\n",
    "* Outer join\n",
    "    * Union of index sets (all labels, no repetition)\n",
    "    * Missing fields filled with NaN\n",
    "* Inner join\n",
    "    * Intersection of index sets (only common labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & inner join\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='inner')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "2860 45199 0.11 34447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & outer join\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='outer')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "1097 NaN 0.33 42.0\n",
    "2860 45199.0 0.11 34447.0\n",
    "37660 40038.0 NaN NaN\n",
    "46167 NaN 0.02 4800.0\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "80808 NaN 0.07 4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join on other axis\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], join='inner', axis=0)\n",
    "\n",
    "Empty DataFrame\n",
    "Columns: []\n",
    "Index: [2860, 46167, 1097, 80808, 57538, 59916, 37660, 2860]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling & concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Merging Data\n",
    "\n",
    "Here, you'll learn all about merging pandas DataFrames. You'll explore different techniques for merging, and learn about left joins, right joins, inner joins, and outer joins, as well as when to use which. You'll also learn about ordered merging, which is useful when you want to merge DataFrames whose columns have natural orderings, like date-time columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Merging DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population DataFrame\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "population = pd.read_csv('pa_zipcode_population.csv')\n",
    "print(population)\n",
    "Zipcode 2010 Census Population\n",
    "0 16855 282\n",
    "1 15681 5241\n",
    "2 18657 11985\n",
    "3 17307 5899\n",
    "4 15635 220\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities DataFrame\n",
    "\n",
    "```python\n",
    "cities = pd.read_csv('pa_zipcode_city.csv')\n",
    "print(cities)\n",
    "Zipcode City State\n",
    "0 17545 MANHEIM PA\n",
    "1 18455 PRESTON PARK PA\n",
    "2 17307 BIGLERVILLE PA\n",
    "3 15705 INDIANA PA\n",
    "4 16833 CURWENSVILLE PA\n",
    "5 16220 CROWN PA\n",
    "6 18618 HARVEYS LAKE PA\n",
    "7 16855 MINERAL SPRINGS PA\n",
    "8 16623 CASSVILLE PA\n",
    "9 15635 HANNASTOWN PA\n",
    "10 15681 SALTSBURG PA\n",
    "11 18657 TUNKHANNOCK PA\n",
    "12 15279 PITTSBURGH PA\n",
    "13 17231 LEMASTERS PA\n",
    "14 18821 GREAT BEND PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "```python\n",
    "pd.merge(population, cities)\n",
    "\n",
    "Zipcode 2010 Census Population City State\n",
    "0 16855 282 MINERAL SPRINGS PA\n",
    "1 15681 5241 SALTSBURG PA\n",
    "2 18657 11985 TUNKHANNOCK PA\n",
    "3 17307 5899 BIGLERVILLE PA\n",
    "4 15635 220 HANNASTOWN PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold)\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on='NOC')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counties DataFrame\n",
    "\n",
    "```python\n",
    "counties = pd.read_csv('pa_counties.csv')\n",
    "print(counties)\n",
    "CITY NAME COUNTY NAME\n",
    "0 SALTSBURG INDIANA\n",
    "1 MINERAL SPRINGS CLEARFIELD\n",
    "2 BIGLERVILLE ADAMS\n",
    "3 HANNASTOWN WESTMORELAND\n",
    "4 TUNKHANNOCK WYOMING\n",
    "\n",
    "print(cities.tail())\n",
    "Zipcode City State\n",
    "10 15681 SALTSBURG PA\n",
    "11 18657 TUNKHANNOCK PA\n",
    "12 15279 PITTSBURGH PA\n",
    "13 17231 LEMASTERS PA\n",
    "14 18821 GREAT BEND PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying columns to merge\n",
    "\n",
    "```python\n",
    "pd.merge(counties, cities, left_on='CITY NAME', right_on='City')\n",
    "\n",
    "CITY NAME COUNTY NAME Zipcode City State\n",
    "0 SALTSBURG INDIANA 15681 SALTSBURG PA\n",
    "1 MINERAL SPRINGS CLEARFIELD 16855 MINERAL SPRINGS PA\n",
    "2 BIGLERVILLE ADAMS 17307 BIGLERVILLE PA\n",
    "3 HANNASTOWN WESTMORELAND 15635 HANNASTOWN PA\n",
    "4 TUNKHANNOCK WYOMING 18657 TUNKHANNOCK PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching left/right DataFrames\n",
    "\n",
    "```python\n",
    "pd.merge(cities, counties, left_on='City', right_on='CITY NAME')\n",
    "\n",
    "Zipcode City State CITY NAME COUNTY NAME\n",
    "0 17307 BIGLERVILLE PA BIGLERVILLE ADAMS\n",
    "1 16855 MINERAL SPRINGS PA MINERAL SPRINGS CLEARFIELD\n",
    "2 15635 HANNASTOWN PA HANNASTOWN WESTMORELAND\n",
    "3 15681 SALTSBURG PA SALTSBURG INDIANA\n",
    "4 18657 TUNKHANNOCK PA TUNKHANNOCK WYOMING\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging company DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on columns with non-matching labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='inner')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with left join\n",
    "\n",
    "* Keeps all rows of the left DF in the merged DF\n",
    "* For rows in the left DF with matches in the right DF:\n",
    "    * Non-joining columns of right DF are appended to left DF\n",
    "* For rows in the left DF with no matches in the right DF:\n",
    "    * Non-joining columns are filled with nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='left')\n",
    "\n",
    "  NOC Country             Total_bronze    Total_gold\n",
    "0 USA United States       1052.0          2088.0\n",
    "1 URS Soviet Union        584.0           838.0\n",
    "2 GBR United Kingdom      505.0           498.0\n",
    "3 FRA France              475.0           NaN\n",
    "4 GER Germany             454.0           407.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with right join\n",
    "\n",
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='right')\n",
    "\n",
    "  NOC Country          Total_bronze    Total_gold\n",
    "0 USA United States    1052.0          2088.0\n",
    "1 URS Soviet Union     584.0           838.0\n",
    "2 GBR United Kingdom   505.0           498.0\n",
    "3 GER Germany          454.0           407.0\n",
    "4 ITA Italy            NaN             460.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with outer join\n",
    "\n",
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='outer')\n",
    "\n",
    "  NOC Country           Total_bronze   Total_gold\n",
    "0 USA United States     1052.0         2088.0\n",
    "1 URS Soviet Union      584.0          838.0\n",
    "2 GBR United Kingdom    505.0          498.0\n",
    "3 FRA France            475.0          NaN\n",
    "4 GER Germany           454.0          407.0\n",
    "5 ITA Italy             NaN            460.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "\n",
    "print(population)\n",
    "                2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538           322\n",
    "59916           130\n",
    "37660           40038\n",
    "2860            45199\n",
    "\n",
    "print(unemployment)\n",
    "         unemployment    participants\n",
    "Zip\n",
    "2860     0.11            34447\n",
    "46167    0.02            4800\n",
    "1097     0.33            42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='left')\n",
    "\n",
    "```python\n",
    "population.join(unemployment)\n",
    "\n",
    "                2010 Census Population unemployment participants\n",
    "Zip Code ZCTA\n",
    "57538           322                    NaN          NaN\n",
    "59916           130                    NaN          NaN\n",
    "37660           40038                  NaN          NaN\n",
    "2860            45199                  0.11         34447.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='right')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='right')\n",
    "\n",
    "        2010 Census Population unemployment participants\n",
    "Zip\n",
    "2860    45199.0                0.11         34447\n",
    "46167   NaN                    0.02         4800\n",
    "1097    NaN                    0.33         42\n",
    "80808   NaN                    0.07         4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='inner')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='inner')\n",
    "\n",
    "        2010 Census Population   unemployment   participants\n",
    "2860                     45199   0.11           34447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='outer')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='outer')\n",
    "\n",
    "        2010 Census Population    unemployment participants\n",
    "1097                   NaN        0.33         42.0\n",
    "2860                   45199.0    0.11         34447.0\n",
    "37660                  40038.0    NaN          NaN\n",
    "46167                  NaN        0.02         4800.0\n",
    "57538                  322.0      NaN          NaN\n",
    "59916                  130.0      NaN          NaN\n",
    "80808                  NaN        0.07         4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which should you use?\n",
    "\n",
    "* df1.append(df2): stacking vertically\n",
    "* pd.concat([df1, df2]):\n",
    "    * stacking many horizontally or vertically\n",
    "    * simple inner/outer joins on Indexes\n",
    "* df1.join(df2): inner/outer/left/right joins on Indexes\n",
    "* pd.merge([df1, df2]): many joins on multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a joining strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left & right merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging DataFrames with outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software & hardware sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software = pd.read_csv(sales_feb_software_file, parse_dates=['Date']).sort_values('Date')\n",
    "software.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = pd.read_csv(sales_feb_hardware_file, parse_dates=['Date']).sort_values('Date')\n",
    "hardware.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software)\n",
    "sales_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer').sort_values('Date')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software)\n",
    "sales_merged.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using on & suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software, on=['Date', 'Company'], suffixes=['_hardware', '_software'])\n",
    "sales_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = data / 'stocks'\n",
    "sp500 = stocks_dir / 'SP500.csv'\n",
    "aapl = stocks_dir/ 'AAPL.csv'\n",
    "csco = stocks_dir/ 'CSCO.csv'\n",
    "amzn = stocks_dir/ 'AMZN.csv'\n",
    "msft = stocks_dir/ 'MSFT.csv'\n",
    "ibm = stocks_dir/ 'IBM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df = pd.read_csv(sp500, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "aapl_df = pd.read_csv(aapl, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "csco_df = pd.read_csv(csco, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "amzn_df = pd.read_csv(amzn, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "msft_df = pd.read_csv(msft, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "ibm_df = pd.read_csv(ibm, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df.rename(columns={'Close': 'S&P'}, inplace=True)\n",
    "aapl_df.rename(columns={'Close': 'AAPL'}, inplace=True)\n",
    "csco_df.rename(columns={'Close': 'CSCO'}, inplace=True)\n",
    "amzn_df.rename(columns={'Close': 'AMZN'}, inplace=True)\n",
    "msft_df.rename(columns={'Close': 'MSFT'}, inplace=True)\n",
    "ibm_df.rename(columns={'Close': 'IBM'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.concat([sp500_df, aapl_df, csco_df, amzn_df, msft_df, ibm_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.to_csv(stocks_dir / 'stocks.csv', index=True, index_label='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.read_csv(gdp_usa_file, parse_dates=['DATE'])\n",
    "gdp.sort_values(by=['DATE'], ascending=False, inplace=True)\n",
    "gdp.reset_index(inplace=True, drop=True)\n",
    "gdp.rename(columns={'VALUE': 'GDP', 'DATE': 'Date'}, inplace=True)\n",
    "gdp.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_2000_2015 = gdp[(gdp['Date'].dt.year >= 2000) & (gdp['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.reset_index(inplace=True)\n",
    "stocks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_2000_2015 = stocks[(stocks['Date'].dt.year >= 2000) & (stocks['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge with ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date', fill_method='ffill')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_asof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Summer Olympics\n",
    "\n",
    "To cement your new skills, you'll apply them by working on an in-depth study involving Olympic medal data. The analysis involves integrating your multi-DataFrame skills from this course and also skills you've gained in previous pandas courses. This is a rich dataset that will allow you to fully leverage your pandas data manipulation skills. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medals in the Summer Olympics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_ioc_codes_file).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_editions_file, sep='\\t').head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summer_1896.csv, summer_1900.csv, …, summer_2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_all_medalists_file, sep='\\t', header=4).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: loading & merging files\n",
    "\n",
    "* pd.read_csv() (& its many options)\n",
    "* Looping over files, e.g.,\n",
    "    * [pd.read_csv(f) for f in glob('*.csv')]\n",
    "* Concatenating & appending, e.g.,\n",
    "    * pd.concat([df1, df2], axis=0)\n",
    "    * df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Olympic edition DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading IOC codes DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building medals DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a pivot table\n",
    "\n",
    "* Apply DataFrame pivot_table() method\n",
    "    * index: column to use as index of pivot table\n",
    "    * values: column(s) to aggregate\n",
    "    * aggfunc: function to apply for aggregation\n",
    "    * columns: categories as columns of pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting medals by country/edition in a pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing fraction of medals per Olympic edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage change in fraction of medals won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building hosts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging to compute influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting influence of host country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
