{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Configuration Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Files Location**\n",
    "\n",
    "* Most data files for the exercises can be found on the [course site](https://www.datacamp.com/courses/merging-dataframes-with-pandas)\n",
    "    * [Baby Names](https://assets.datacamp.com/production/repositories/516/datasets/43c9b6bf4c283ab024b2d7d61fbf15a0baa1e44d/Baby%20names.zip)\n",
    "    * [Summer Olympic Medals](https://assets.datacamp.com/production/repositories/516/datasets/2d14df8d3c6a1773358fa000f203282c2e1107d6/Summer%20Olympic%20medals.zip)\n",
    "    * [Automobile Fuel Efficiency](https://assets.datacamp.com/production/repositories/516/datasets/2f3d8b2156d5669fb7e12137f1c2e979c3c9ce0b/automobiles.csv)\n",
    "    * [Exchange Rates](https://assets.datacamp.com/production/repositories/516/datasets/e91482db6a7bae394653278e4e908e63ed9ac833/exchange.csv)\n",
    "    * [GDP](https://assets.datacamp.com/production/repositories/516/datasets/a0858a700501f88721ca9e4bdfca99b9e10b937f/GDP.zip)\n",
    "    * [Oil Prices](https://assets.datacamp.com/production/repositories/516/datasets/707566cf46c4dd6290b9029f5e07a92baf3fe3f7/oil_price.csv)\n",
    "    * [Pittsburgh Weather](https://assets.datacamp.com/production/repositories/516/datasets/58c1ead59818b2451324e9e84239db7bda6b11d3/pittsburgh2013.csv)\n",
    "    * [Sales](https://assets.datacamp.com/production/repositories/516/datasets/2b89c1b00016e1ebcfd7f08a127d2c79589ce5c0/Sales.zip)\n",
    "    * [S&P 500](https://assets.datacamp.com/production/repositories/516/datasets/7a9b570a02ef589891d9576a86876a616ca5f3c8/sp500.csv)\n",
    "* Other data files may be found in my [DataCamp repository](https://github.com/trenton3983/DataCamp/tree/master/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data File Objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / 'data'\n",
    "auto_fuel_file = data / 'merging-dataframes-with-pandas' / 'auto_fuel_efficiency.csv'\n",
    "baby_1881_file = data / 'merging-dataframes-with-pandas' / 'baby_names1881.csv'\n",
    "baby_1981_file = data / 'merging-dataframes-with-pandas' / 'baby_names1981.csv'\n",
    "exch_rates_file = data / 'merging-dataframes-with-pandas' / 'exchange_rates.csv'\n",
    "gdp_china_file = data / 'merging-dataframes-with-pandas' / 'gdp_china.csv'\n",
    "gdp_usa_file = data / 'merging-dataframes-with-pandas' / 'gdp_usa.csv'\n",
    "oil_price_file = data / 'merging-dataframes-with-pandas' / 'oil_price.csv'\n",
    "pitts_file = data / 'merging-dataframes-with-pandas' / 'pittsburgh_weather_2013.csv'\n",
    "sales_feb_hardware_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Hardware.csv'\n",
    "sales_feb_service_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Service.csv'\n",
    "sales_feb_software_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-Software.csv'\n",
    "sales_jan_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-jan-2015.csv'\n",
    "sales_feb_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-feb-2015.csv'\n",
    "sales_mar_2015_file = data / 'merging-dataframes-with-pandas' / 'sales-mar-2015.csv'\n",
    "sp500_file = data / 'merging-dataframes-with-pandas' / 'sp500.csv'\n",
    "so_bronze_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Bronze.csv'\n",
    "so_bronze5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_bronze_top5.csv'\n",
    "so_gold_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Gold.csv'\n",
    "so_gold5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_gold_top5.csv'\n",
    "so_silver_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_Silver.csv'\n",
    "so_silver5_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_silver_top5.csv'\n",
    "so_all_medalists_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - ALL MEDALISTS.tsv'\n",
    "so_editions_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - EDITIONS.tsv'\n",
    "so_ioc_codes_file = data / 'merging-dataframes-with-pandas' / 'summer_olympics_medalists 1896 to 2008 - IOC COUNTRY CODES.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Merging DataFrames with pandas\n",
    "\n",
    "***Course Description***\n",
    "\n",
    "As a Data Scientist, you'll often find that the data you need is not in a single file. It may be spread across a number of text files, spreadsheets, or databases. You want to be able to import the data of interest as a collection of DataFrames and figure out how to combine them to answer your central questions. This course is all about the act of combining, or merging, DataFrames, an essential part of any working Data Scientist's toolbox. You'll hone your pandas skills by learning how to organize, reshape, and aggregate multiple data sets to answer your specific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "In this chapter, you'll learn about different techniques you can use to import multiple files into DataFrames. Having imported your data into individual DataFrames, you'll then learn how to share information between DataFrames using their Indexes. Understanding how Indexes work is essential information that you'll need for merging DataFrames later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Reading multiple data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools for pandas data import\n",
    "\n",
    "* pd.read_csv() for CSV files\n",
    "    * dataframe = pd.read_csv(filepath)\n",
    "    * dozens of optional input parameters\n",
    "* Other data import tools:\n",
    "    * pd.read_excel()\n",
    "    * pd.read_html()\n",
    "    * pd.read_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading separate files\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "dataframe0 = pd.read_csv('sales-jan-2015.csv')\n",
    "dataframe1 = pd.read_csv('sales-feb-2015.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a loop\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = []\n",
    "for f in filenames:\n",
    "    dataframes.append(pd.read_csv(f))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a comprehension\n",
    "\n",
    "```python\n",
    "filenames = ['sales-jan-2015.csv', 'sales-feb-2015.csv']\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using glob\n",
    "\n",
    "```python\n",
    "from glob import glob\n",
    "filenames = glob('sales*.csv')\n",
    "dataframes = [pd.read_csv(f) for f in filenames]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files\n",
    "\n",
    "When data is spread among several files, you usually invoke pandas' <code>read_csv()</code> (or a similar data import function) multiple times to load the data into several DataFrames.\n",
    "\n",
    "The data files for this example have been derived from a [list of Olympic medals awarded between 1896 & 2008](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data) compiled by the Guardian.\n",
    "\n",
    "The column labels of each DataFrame are <code>NOC</code>, <code>Country</code>, & <code>Total</code> where <code>NOC</code> is a three-letter code for the name of the country and <code>Total</code> is the number of medals of that type won (bronze, silver, or gold).\n",
    "\n",
    "**Instructions**\n",
    "<ul>\n",
    "<li>Import <span style=\"background-color: #A733FF\">pandas</span> as <span style=\"background-color: #A733FF\">pd</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Bronze.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">bronze</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Silver.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">silver</span>.</li>\n",
    "<li>Read the file <span style=\"background-color: #A733FF\">'Gold.csv'</span> into a DataFrame called <span style=\"background-color: #A733FF\">gold</span>.</li>\n",
    "<li>Print the first 5 rows of the DataFrame <span style=\"background-color: #A733FF\">gold</span>. This has been done for you, so hit 'Submit Answer' to see the results.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'Bronze.csv' into a DataFrame: bronze\n",
    "bronze = pd.read_csv(so_bronze_file)\n",
    "\n",
    "# Read 'Silver.csv' into a DataFrame: silver\n",
    "silver = pd.read_csv(so_silver_file)\n",
    "\n",
    "# Read 'Gold.csv' into a DataFrame: gold\n",
    "gold = pd.read_csv(so_gold_file)\n",
    "\n",
    "# Print the first five rows of gold\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading DataFrames from multiple files in a loop\n",
    "\n",
    "As you saw in the video, loading data from multiple files into DataFrames is more efficient in a <em>loop</em> or a <em>list comprehension</em>.\n",
    "\n",
    "Notice that this approach is not restricted to working with CSV files. That is, even if your data comes in other formats, as long as pandas has a suitable data import function, you can apply a loop or comprehension to generate a list of DataFrames imported from the source files.\n",
    "\n",
    "Here, you'll continue working with [The Guardian's Olympic medal dataset](https://www.theguardian.com/sport/datablog/2012/jun/25/olympic-medal-winner-list-data).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a list of file names called <mark>filenames</mark> with three strings <mark>'Gold.csv'</mark>, <mark>'Silver.csv'</mark>, & <mark>'Bronze.csv'</mark>. This has been done for you.\n",
    "* Use a <mark>for</mark> loop to create another list called <mark>dataframes</mark> containing the three DataFrames loaded from <mark>filenames</mark>:\n",
    "    * Iterate over <mark>filenames</mark>.\n",
    "    * Read each CSV file in <mark>filenames</mark> into a DataFrame and append it to <mark>dataframes</mark> by using <mark>pd.read_csv()</mark> inside a call to <mark>.append()</mark>.\n",
    "* Print the first 5 rows of the first DataFrame of the list <mark>dataframes</mark>. This has been done for you, so hit 'Submit Answer' to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of file names: filenames\n",
    "filenames = [so_bronze_file, so_silver_file, so_gold_file]\n",
    "\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining DataFrames from multiple data files\n",
    "\n",
    "In this exercise, you'll *combine* the three DataFrames from earlier exercises - ```gold```, ```silver```, & ```bronze``` - into a single DataFrame called ```medals```. The approach you'll use here is clumsy. Later on in the course, you'll see various powerful methods that are frequently used in practice for *concatenating* or *merging* DataFrames.\n",
    "\n",
    "Remember, the column labels of each DataFrame are ```NOC```, ```Country```, and ```Total```, where ```NOC``` is a three-letter code for the name of the country and ```Total``` is the number of medals of that type won.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Construct a copy of the DataFrame ```gold``` called ```medals``` using the ```.copy()``` method.\n",
    "* Create a list called ```new_labels``` with entries ```'NOC'```, ```'Country'```, & ```'Gold'```. This is the same as the column labels from ```gold``` with the column label ```'Total'``` replaced by ```'Gold'```.\n",
    "* Rename the columns of ```medals``` by assigning ```new_labels``` to ```medals.columns```.\n",
    "* Create new columns ```'Silver'``` and ```'Bronze'``` in medals using ```silver['Total']``` & ```bronze['Total']```.\n",
    "* Print the top 5 rows of the final DataFrame ```medals```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver['Total']\n",
    "medals['Bronze'] = bronze['Total']\n",
    "\n",
    "# Print the head of medals\n",
    "medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bronze, silver, gold, dataframes, medals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Indexes\" vs. \"Indices\"\n",
    "\n",
    "* indices: many index labels within Index data structures\n",
    "* indexes: many pandas Index data structures\n",
    "\n",
    "![](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/merging_dataframes_in_python/indices_indexes.JPG \"Indices & Indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "w_mean = pd.read_csv('quarterly_mean_temp.csv', index_col='Month')\n",
    "w_max = pd.read_csv('quarterly_max_temp.csv', index_col='Month')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the data\n",
    "\n",
    "```python\n",
    "print(w_mean)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "\n",
    "print(w_max)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68\n",
    "Apr     89\n",
    "Jul     91\n",
    "Oct     84\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame indexes\n",
    "\n",
    "```python\n",
    "print(w_mean.index)\n",
    "Index(['Apr', 'Jan', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(w_max.index)\n",
    "Index(['Jan', 'Apr', 'Jul', 'Oct'], dtype='object', name='Month')\n",
    "\n",
    "print(type(w_mean.index))\n",
    "<class 'pandas.indexes.base.Index'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reindex()\n",
    "\n",
    "```python\n",
    "ordered = ['Jan', 'Apr', 'Jul', 'Oct']\n",
    "w_mean2 = w_mean.reindex(ordered)\n",
    "print(w_mean2)\n",
    "\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .sort_index()\n",
    "\n",
    "```python\n",
    "w_mean2.sort_index()\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Apr     61.956044\n",
    "Jan     32.133333\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing with missing labels\n",
    "\n",
    "```python\n",
    "w_mean3 = w_mean.reindex(['Jan', 'Apr', 'Dec'])\n",
    "print(w_mean3)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Dec     NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindex from a DataFrame Index\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean3.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "Dec     NaN\n",
    "\n",
    "w_max.reindex(w_mean3.index).dropna()\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Jan     68.0\n",
    "Apr     89.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order matters\n",
    "\n",
    "```python\n",
    "w_max.reindex(w_mean.index)\n",
    "        Max TemperatureF\n",
    "Month\n",
    "Apr     89\n",
    "Jan     68\n",
    "Jul     91\n",
    "Oct     84\n",
    "\n",
    "w_mean.reindex(w_max.index)\n",
    "        Mean TemperatureF\n",
    "Month\n",
    "Jan     32.133333\n",
    "Apr     61.956044\n",
    "Jul     68.934783\n",
    "Oct     43.434783\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting DataFrame with the Index & columns\n",
    "\n",
    "It is often useful to rearrange the sequence of the rows of a DataFrame by *sorting*. You don't have to implement these yourself; the principal methods for doing this are ```.sort_index()``` and ```.sort_values()```.\n",
    "\n",
    "In this exercise, you'll use these methods with a DataFrame of temperature values indexed by month names. You'll sort the rows alphabetically using the Index and numerically using a column. Notice, for this data, the original ordering is probably most useful and intuitive: the purpose here is for you to understand what the sorting methods do.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read ```'monthly_max_temp.csv'``` into a DataFrame called ```weather1``` with ```'Month'``` as the index.\n",
    "* Sort the index of ```weather1``` in alphabetical order using the ```.sort_index()``` method and store the result in ```weather2```.\n",
    "* Sort the index of ```weather1``` in *reverse* alphabetical order by specifying the additional keyword argument ```ascending=False``` inside ```.sort_index()```.\n",
    "* Use the ```.sort_values()``` method to sort ```weather1``` in increasing numerical order according to the values of the column ```'Max TemperatureF'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_max_temp = {'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "                    'Max TemperatureF': [68, 60, 68, 84, 88, 89, 91, 86, 90, 84, 72, 68]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "# weather1 = pd.read_csv('monthly_max_temp.csv', index_col='Month')\n",
    "weather1 = pd.DataFrame.from_dict(monthly_max_temp)\n",
    "weather1.set_index('Month', inplace=True)\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values(by='Max TemperatureF')\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame from a list\n",
    "\n",
    "Sorting methods are not the only way to change DataFrame Indexes. There is also the ```.reindex()``` method.\n",
    "\n",
    "In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples (this is an example of *upsampling* or increasing the rate of samples, which you may recall from the [pandas Foundations](https://www.datacamp.com/courses/pandas-foundations) course).\n",
    "\n",
    "The original data has the first month's abbreviation of the quarter (three-month interval) on the Index, namely ```Apr```, ```Jan```, ```Jul```, and ```Oct```. This data has been loaded into a DataFrame called ```weather1``` and has been printed in its entirety in the IPython Shell. Notice it has only four rows (corresponding to the first month of each quarter) and that the rows are not sorted chronologically.\n",
    "\n",
    "You'll initially use a list of all twelve month abbreviations and subsequently apply the ```.ffill()``` method to *forward-fill* the null entries when upsampling. This list of month abbreviations has been pre-loaded as ```year```.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Reorder the rows of ```weather1``` using the ```.reindex()``` method with the list ```year``` as the argument, which contains the abbreviations for each month.\n",
    "* Reorder the rows of ```weather1``` just as you did above, this time chaining the ```.ffill()``` method to replace the null values with the last preceding non-null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_max_temp = {'Month': ['Jan', 'Apr', 'Jul', 'Oct'],\n",
    "                    'Max TemperatureF': [32.13333, 61.956044, 68.934783, 43.434783]}\n",
    "weather1 = pd.DataFrame.from_dict(monthly_max_temp)\n",
    "weather1.set_index('Month', inplace=True)\n",
    "weather1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2\n",
    "weather2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "weather3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reindexing DataFrame using another DataFrame Index\n",
    "\n",
    "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame ```.reindex()``` method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its ```.index``` attribute.\n",
    "\n",
    "The [Baby Names Dataset](https://www.data.gov/developers/baby-names-dataset/) from [data.gov](http://data.gov/) summarizes counts of names (with genders) from births registered in the US since 1881. In this exercise, you will start with two baby-names DataFrames ```names_1981``` and ```names_1881``` loaded for you.\n",
    "\n",
    "The DataFrames ```names_1981``` and ```names_1881``` both have a MultiIndex with levels ```name``` and ```gender``` giving unique labels to counts in each row. If you're interested in seeing how the MultiIndexes were set up, ```names_1981``` and ```names_1881``` were read in using the following commands:\n",
    "\n",
    "```python\n",
    "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "```\n",
    "\n",
    "As you can see by looking at their shapes, which have been printed in the IPython Shell, the DataFrame corresponding to 1981 births is much larger, reflecting the greater diversity of names in 1981 as compared to 1881.\n",
    "\n",
    "Your job here is to use the DataFrame ```.reindex()``` and ```.dropna()``` methods to make a DataFrame ```common_names``` counting names from 1881 that were still popular in 1981.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame ```common_names``` by reindexing ```names_1981``` using the Index of the DataFrame ```names_1881``` of older names.\n",
    "* Print the shape of the new ```common_names``` DataFrame. This has been done for you. It should be the same as that of ```names_1881```.\n",
    "* Drop the rows of ```common_names``` that have null counts using the ```.dropna()``` method. These rows correspond to names that fell out of fashion between 1881 & 1981.\n",
    "* Print the shape of the reassigned ```common_names``` DataFrame. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1981 = pd.read_csv(baby_1981_file, header=None, names=['name', 'gender', 'count'], index_col=(0,1))\n",
    "names_1981.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_1881 = pd.read_csv(baby_1881_file, header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "common_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "common_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_names.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather1, weather2, weather3, weather4, common_names, names_1881, names_1981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic with Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading weather data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "weather = pd.read_csv('pittsburgh2013.csv', index_col='Date', parse_dates=True)\n",
    "weather.loc['2013-7-1':'2013-7-7', 'PrecipitationIn']\n",
    "\n",
    "Date\n",
    "2013-07-01 0.18\n",
    "2013-07-02 0.14\n",
    "2013-07-03 0.00\n",
    "2013-07-04 0.25\n",
    "2013-07-05 0.02\n",
    "2013-07-06 0.06\n",
    "2013-07-07 0.10\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar multiplication\n",
    "\n",
    "```python\n",
    "weather.loc['2013-07-01':'2013-07-07', 'PrecipitationIn'] * 2.54\n",
    "\n",
    "Date\n",
    "2013-07-01 0.4572\n",
    "2013-07-02 0.3556\n",
    "2013-07-03 0.0000\n",
    "2013-07-04 0.6350\n",
    "2013-07-05 0.0508\n",
    "2013-07-06 0.1524\n",
    "2013-07-07 0.2540\n",
    "Name: PrecipitationIn, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute temperature range\n",
    "\n",
    "```python\n",
    "week1_range = weather.loc['2013-07-01':'2013-07-07', ['Min TemperatureF', 'Max TemperatureF']]\n",
    "print(week1_range)\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 66 79\n",
    "2013-07-02 66 84\n",
    "2013-07-03 71 86\n",
    "2013-07-04 70 86\n",
    "2013-07-05 69 86\n",
    "2013-07-06 70 89\n",
    "2013-07-07 70 77\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average temperature\n",
    "\n",
    "```python\n",
    "week1_mean = weather.loc['2013-07-01':'2013-07-07', 'Mean TemperatureF']\n",
    "print(week1_mean)\n",
    "Date\n",
    "2013-07-01 72\n",
    "2013-07-02 74\n",
    "2013-07-03 78\n",
    "2013-07-04 77\n",
    "2013-07-05 76\n",
    "2013-07-06 78\n",
    "2013-07-07 72\n",
    "Name: Mean TemperatureF, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range / week1_mean\n",
    "RuntimeWarning: Cannot compare type 'Timestamp' with type 'str', sort order is\n",
    "undefined for incomparable objects\n",
    "return this.join(other, how=how, return_indexers=return_indexers)\n",
    "\n",
    "2013-07-01 00:00:00 2013-07-02 00:00:00 2013-07-03 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "2013-07-02 NaN NaN NaN\n",
    "2013-07-03 NaN NaN NaN\n",
    "2013-07-04 NaN NaN NaN\n",
    "2013-07-05 NaN NaN NaN\n",
    "2013-07-06 NaN NaN NaN\n",
    "2013-07-07 NaN NaN NaN\n",
    "2013-07-04 00:00:00 2013-07-05 00:00:00 2013-07-06 00:00:00 \\\n",
    "Date\n",
    "2013-07-01 NaN NaN NaN\n",
    "... ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative temperature range\n",
    "\n",
    "```python\n",
    "week1_range.divide(week1_mean, axis='rows')\n",
    "\n",
    "Min TemperatureF Max TemperatureF\n",
    "Date\n",
    "2013-07-01 0.916667 1.097222\n",
    "2013-07-02 0.891892 1.135135\n",
    "2013-07-03 0.910256 1.102564\n",
    "2013-07-04 0.909091 1.116883\n",
    "2013-07-05 0.907895 1.131579\n",
    "2013-07-06 0.897436 1.141026\n",
    "2013-07-07 0.972222 1.069444\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage changes\n",
    "\n",
    "```python\n",
    "week1_mean.pct_change() * 100\n",
    "\n",
    "Date\n",
    "2013-07-01 NaN\n",
    "2013-07-02 2.777778\n",
    "2013-07-03 5.405405\n",
    "2013-07-04 -1.282051\n",
    "2013-07-05 -1.298701\n",
    "2013-07-06 2.631579\n",
    "2013-07-07 -7.692308\n",
    "Name: Mean TemperatureF, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bronze Olympic medals\n",
    "\n",
    "```python\n",
    "bronze = pd.read_csv('bronze_top5.csv', index_col=0)\n",
    "print(bronze)\n",
    "Total\n",
    "Country\n",
    "United States 1052.0\n",
    "Soviet Union 584.0\n",
    "United Kingdom 505.0\n",
    "France 475.0\n",
    "Germany 454.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver Olympic medals\n",
    "\n",
    "```python\n",
    "silver = pd.read_csv('silver_top5.csv', index_col=0)\n",
    "print(silver)\n",
    "Total\n",
    "Country\n",
    "United States 1195.0\n",
    "Soviet Union 627.0\n",
    "United Kingdom 591.0\n",
    "France 461.0\n",
    "Italy 394.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Olympic medals\n",
    "\n",
    "```python\n",
    "gold = pd.read_csv('gold_top5.csv', index_col=0)\n",
    "print(gold)\n",
    "Total\n",
    "Country\n",
    "United States 2088.0\n",
    "Soviet Union 838.0\n",
    "United Kingdom 498.0\n",
    "Italy 460.0\n",
    "Germany 407.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver\n",
    "\n",
    "```python\n",
    "bronze + silver\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "In [22]: print(bronze['United States'])\n",
    "1052.0\n",
    "In [23]: print(silver['United States'])\n",
    "1195.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the .add() method\n",
    "\n",
    "```python\n",
    "bronze.add(silver)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a fill_value\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 454.0\n",
    "Italy 394.0\n",
    "Soviet Union 1211.0\n",
    "United Kingdom 1096.0\n",
    "United States 2247.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding bronze, silver, gold\n",
    "\n",
    "```python\n",
    "bronze + silver + gold\n",
    "\n",
    "Country\n",
    "France NaN\n",
    "Germany NaN\n",
    "Italy NaN\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chaining .add()\n",
    "\n",
    "```python\n",
    "bronze.add(silver, fill_value=0).add(gold, fill_value=0)\n",
    "\n",
    "Country\n",
    "France 936.0\n",
    "Germany 861.0\n",
    "Italy 854.0\n",
    "Soviet Union 2049.0\n",
    "United Kingdom 1594.0\n",
    "United States 4335.0\n",
    "Name: Total, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding unaligned DataFrames\n",
    "\n",
    "The DataFrames ```january``` and ```february```, which have been printed in the IPython Shell, represent the sales a company made in the corresponding months.\n",
    "\n",
    "The Indexes in both DataFrames are called ```Company```, identifying which company bought that quantity of units. The column ```Units``` is the number of units sold.\n",
    "\n",
    "If you were to add these two ```DataFrames``` by executing the command ```total = january + february```, how many rows would the resulting DataFrame have? Try this in the IPython Shell and find out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_dict = {'Company': ['Acme Corporation', 'Hooli', 'Initech', 'Mediacore', 'Streeplex'],\n",
    "            'Units': [19, 17, 20, 10, 13]}\n",
    "feb_dict = {'Company': ['Acme Corporation', 'Hooli', 'Mediacore', 'Vandelay Inc'],\n",
    "            'Units': [15, 3, 12, 25]}\n",
    "\n",
    "january = pd.DataFrame.from_dict(jan_dict)\n",
    "january.set_index('Company', inplace=True)\n",
    "print(january)\n",
    "\n",
    "february = pd.DataFrame.from_dict(feb_dict)\n",
    "february.set_index('Company', inplace=True)\n",
    "print('\\n', february, '\\n')\n",
    "\n",
    "print(january + february)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting in Arithmetic formulas\n",
    "\n",
    "In this exercise, you'll work with weather data pulled from [wunderground.com](https://www.wunderground.com/). The DataFrame <code>weather</code> has been pre-loaded along with <code>pandas as pd</code>. It has 365 rows (observed each day of the year 2013 in Pittsburgh, PA) and 22 columns reflecting different weather measurements each day.\n",
    "\n",
    "You'll subset a collection of columns related to temperature measurements in degrees Fahrenheit, convert them to degrees Celsius, and relabel the columns of the new DataFrame to reflect the change of units.\n",
    "\n",
    "Remember, ordinary arithmetic operators (like <code>+</code>, <code>-</code>, <code>*</code>, and <code>/</code>) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Create a new DataFrame ```temps_f``` by extracting the columns ```'Min TemperatureF'```, ```'Mean TemperatureF'```, & ```'Max TemperatureF'``` from ```weather``` as a new DataFrame ```temps_f```. To do this, pass the relevant columns as a list to ```weather[]```.\n",
    "* Create a new DataFrame ```temps_c``` from ```temps_f``` using the formula ```(temps_f - 32) * 5/9```.\n",
    "* Rename the columns of ```temps_c``` to replace ```'F'``` with ```'C'``` using the ```.str.replace('F', 'C')``` method on ```temps_c.columns```.\n",
    "* Print the first 5 rows of DataFrame ```temps_c```. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(pitts_file)\n",
    "weather.set_index('Date', inplace=True)\n",
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected columns from weather as new DataFrame: temps_f\n",
    "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f - 32) * 5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns = temps_c.columns.str.replace('F', 'C')\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "temps_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage growth of GDP\n",
    "\n",
    "Your job in this exercise is to compute the yearly percent-change of US GDP ([Gross Domestic Product](https://en.wikipedia.org/wiki/Gross_domestic_product)) since 2008.\n",
    "\n",
    "The data has been obtained from the [Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/series/GDP/downloaddata) and is available in the file ```GDP.csv```, which contains quarterly data; you will resample it to annual sampling and then compute the annual growth of GDP. For a refresher on resampling, check out the relevant material from [pandas Foundations](https://campus.datacamp.com/courses/pandas-foundations/time-series-in-pandas?ex=7).\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read the file ```'GDP.csv'``` into a DataFrame called ```gdp```.\n",
    "* Use ```parse_dates=True``` and ```index_col='DATE'```.\n",
    "* Create a DataFrame ```post2008``` by slicing ```gdp``` such that it comprises all rows from 2008 onward.\n",
    "* Print the last 8 rows of the slice ```post2008```. This has been done for you. This data has quarterly frequency so the indices are separated by three-month intervals.\n",
    "* Create the DataFrame ```yearly``` by resampling the slice ```post2008``` by year. Remember, you need to chain ```.resample()``` (using the alias ```'A'``` for annual frequency) with some kind of aggregation; you will use the aggregation method ```.last()``` to select the last element when resampling.\n",
    "* Compute the percentage growth of the resampled DataFrame ```yearly``` with ```.pct_change() * 100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv(gdp_usa_file, parse_dates=True, index_col='DATE')\n",
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp.loc['2008-01-01':]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "post2008.tail(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample('A').last()\n",
    "\n",
    "# Print yearly\n",
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change() * 100\n",
    "\n",
    "# Print yearly again\n",
    "yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting currency of stocks\n",
    "\n",
    "In this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from [Yahoo Finance](https://finance.yahoo.com/). The files ```sp500.csv``` for sp500 and ```exchange.csv``` for the exchange rates are both provided to you.\n",
    "\n",
    "Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Read the DataFrames ```sp500``` & ```exchange``` from the files ```'sp500.csv'``` & ```'exchange.csv'``` respectively..\n",
    "* Use ```parse_dates=True``` and ```index_col='Date'```.\n",
    "* Extract the columns ```'Open'``` & ```'Close'``` from the DataFrame ```sp500``` as a new DataFrame ```dollars``` and print the first 5 rows.\n",
    "* Construct a new DataFrame ```pounds``` by converting US dollars to British pounds. You'll use the ```.multiply()``` method of ```dollars``` with ```exchange['GBP/USD']``` and ```axis='rows'```\n",
    "* Print the first 5 rows of the new DataFrame ```pounds```. This has been done for you, so hit 'Submit Answer' to see the results!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv(sp500_file, parse_dates=True, index_col='Date')\n",
    "sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv(exch_rates_file, parse_dates=True, index_col='Date')\n",
    "exchange.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[['Open', 'Close']]\n",
    "\n",
    "# Print the head of dollars\n",
    "dollars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange['GBP/USD'], axis='rows')\n",
    "\n",
    "# Print the head of pounds\n",
    "pounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del january, february, feb_dict, jan_dict, weather, temps_f, temps_c, gdp, post2008, yearly, sp500, exchange, dollars, pounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Data\n",
    "\n",
    "Having learned how to import multiple DataFrames and share information using Indexes, in this chapter you'll learn how to perform database-style operations to combine DataFrames. In particular, you'll learn about appending and concatenating DataFrames while working with a variety of real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append()\n",
    "\n",
    "* .append(): Series & DataFrame method\n",
    "* Invocation:\n",
    "* s1.append(s2)\n",
    "* Stacks rows of s2 below s1\n",
    "* Method for Series & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat()\n",
    "\n",
    "* concat(): pandas module function\n",
    "* Invocation:\n",
    "* pd.concat([s1, s2, s3])\n",
    "* Can stack row-wise or column-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat() & .append()\n",
    "\n",
    "* Equivalence of concat() & .append():\n",
    "* result1 = pd.concat([s1, s2, s3])\n",
    "* result2 = s1.append(s2).append(s3)\n",
    "* result1 == result2 elementwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series of US states\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "northeast = pd.Series(['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'])\n",
    "south = pd.Series(['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'])\n",
    "midwest = pd.Series(['IL', 'IN', 'MN', 'MO', 'NE', 'ND', 'SD', 'IA', 'KS', 'MI', 'OH', 'WI'])\n",
    "west = pd.Series(['AZ', 'CO', 'ID', 'MT',\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .append()\n",
    "\n",
    "```python\n",
    "east = northeast.append(south)\n",
    "print(east)\n",
    "0 CT       7 DC\n",
    "1 ME       8 WV\n",
    "2 MA       9 AL\n",
    "3 NH       10 KY\n",
    "4 RI       11 MS\n",
    "5 VT       12 TN\n",
    "6 NJ       13 AR\n",
    "7 NY       14 LA\n",
    "8 PA       15 OK\n",
    "0 DE       16 TX\n",
    "1 FL       dtype: object\n",
    "2 GA\n",
    "3 MD\n",
    "4 NC\n",
    "5 SC\n",
    "6 VA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The appended Index\n",
    "\n",
    "```python\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "\n",
    "print(east.loc[3])\n",
    "3 NH\n",
    "3 MD\n",
    "dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .reset_index()\n",
    "\n",
    "```python\n",
    "new_east = northeast.append(south).reset_index(drop=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using concat()\n",
    "\n",
    "```python\n",
    "east = pd.concat([northeast, south])\n",
    "print(east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "0 DE\n",
    "1 FL\n",
    "dtype: object\n",
    "print(east.index)\n",
    "Int64Index([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ignore_index\n",
    "\n",
    "```python\n",
    "new_east = pd.concat([northeast, south], ignore_index=True)\n",
    "print(new_east.head(11))\n",
    "0 CT\n",
    "1 ME\n",
    "2 MA\n",
    "3 NH\n",
    "4 RI\n",
    "5 VT\n",
    "6 NJ\n",
    "7 NY\n",
    "8 PA\n",
    "9 DE\n",
    "10 FL\n",
    "dtype: object\n",
    "print(new_east.index)\n",
    "RangeIndex(start=0, stop=26, step=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending Series with nonunique Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading population data\n",
    "\n",
    "```python\n",
    "In [1]: import pandas as pd\n",
    "In [2]: pop1 = pd.read_csv('population_01.csv', index_col=0)\n",
    "In [3]: pop2 = pd.read_csv('population_02.csv', index_col=0)\n",
    "In [4]: print(type(pop1), pop1.shape)\n",
    "<class 'pandas.core.frame.DataFrame'> (4, 1)\n",
    "In [5]: print(type(pop2), pop2.shape)\n",
    "<class 'pandas.core.frame.DataFrame'> (4, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining population data\n",
    "\n",
    "```python\n",
    "print(pop1)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "66407 479\n",
    "72732 4716\n",
    "50579 2405\n",
    "46241 30670\n",
    "print(pop2)\n",
    "\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "12776 2180\n",
    "76092 26669\n",
    "98360 12221\n",
    "49464 27481\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population DataFrames\n",
    "\n",
    "```python\n",
    "pop1.append(pop2)\n",
    "\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "66407 479\n",
    "72732 4716\n",
    "50579 2405\n",
    "46241 30670\n",
    "12776 2180\n",
    "76092 26669\n",
    "98360 12221\n",
    "49464 27481\n",
    "\n",
    "print(pop1.index.name, pop1.columns)\n",
    "Zip Code ZCTA Index(['2010 Census Population'], dtype='object')\n",
    "\n",
    "print(pop2.index.name, pop2.columns)\n",
    "Zip Code ZCTA Index(['2010 Census Population'], dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "print(population)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538 322\n",
    "59916 130\n",
    "37660 40038\n",
    "2860 45199\n",
    "\n",
    "print(unemployment)\n",
    "unemployment participants\n",
    "Zip\n",
    "2860 0.11 34447\n",
    "46167 0.02 4800\n",
    "1097 0.33 42\n",
    "80808 0.07 4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending population & unemployment\n",
    "\n",
    "```python\n",
    "population.append(unemployment)\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated index labels\n",
    "\n",
    "```python\n",
    "population.append(unemployment)\n",
    "\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=0)\n",
    "\n",
    "2010 Census Population participants unemployment\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "37660 40038.0 NaN NaN\n",
    "2860 45199.0 NaN NaN\n",
    "2860 NaN 34447.0 0.11\n",
    "46167 NaN 4800.0 0.02\n",
    "1097 NaN 42.0 0.33\n",
    "80808 NaN 4310.0 0.07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating columns\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1)\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "1097 NaN 0.33 42.0\n",
    "2860 45199.0 0.11 34447.0\n",
    "37660 40038.0 NaN NaN\n",
    "46167 NaN 0.02 4800.0\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "80808 NaN 0.07 4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending DataFrames with ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating pandas DataFrames along column axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation, keys & MultiIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading rainfall data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "file1 = 'q1_rainfall_2013.csv'\n",
    "rain2013 = pd.read_csv(file1, index_col='Month', parse_dates=True)\n",
    "file2 = 'q1_rainfall_2014.csv'\n",
    "rain2014 = pd.read_csv(file2, index_col='Month', parse_dates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining rainfall data\n",
    "\n",
    "```python\n",
    "print(rain2013)\n",
    "Precipitation\n",
    "Month\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "\n",
    "print(rain2014)\n",
    "Precipitation\n",
    "Month\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating rows\n",
    "\n",
    "```python\n",
    "pd.concat([rain2013, rain2014], axis=0)\n",
    "\n",
    "Precipitation\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using multi-index on rows\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis=0)\n",
    "print(rain1314)\n",
    "Precipitation\n",
    "2013 Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "2014 Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing a multi-index\n",
    "\n",
    "```python\n",
    "print(rain1314.loc[2014])\n",
    "Precipitation\n",
    "Jan 0.050323\n",
    "Feb 0.082143\n",
    "Mar 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating columns\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], axis='columns')\n",
    "print(rain1314)\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a multi-index on columns\n",
    "\n",
    "```python\n",
    "rain1314 = pd.concat([rain2013, rain2014], keys=[2013, 2014], axis='columns')\n",
    "print(rain1314)\n",
    "2013 2014\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "\n",
    "print(rain1314[2013])\n",
    "Precipitation\n",
    "Jan 0.096129\n",
    "Feb 0.067143\n",
    "Mar 0.061613\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.concat() with dict\n",
    "\n",
    "```python\n",
    "rain_dict = {2013: rain2013, 2014: rain2014}\n",
    "rain1314 = pd.concat(rain_dict, axis='columns')\n",
    "print(rain1314)\n",
    "2013 2014\n",
    "Precipitation Precipitation\n",
    "Jan 0.096129 0.050323\n",
    "Feb 0.067143 0.082143\n",
    "Mar 0.061613 0.070968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating vertically to get MultiIndexed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing MultiIndexed DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating horizontally to get MultiIndexed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames from a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer & inner joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using with arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(8).reshape(2, 4) + 0.1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.arange(6).reshape(2,3) + 0.2\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(12).reshape(3,4) + 0.3\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([B, A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([B, A], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking arrays vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([A, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incompatible array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, B], axis=0) # incompatible columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([A, C], axis=1) # incompatible rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "print(population)\n",
    "2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538 322\n",
    "59916 130\n",
    "37660 40038\n",
    "2860 45199\n",
    "\n",
    "print(unemployment)\n",
    "unemployment participants\n",
    "Zip\n",
    "2860 0.11 34447\n",
    "46167 0.02 4800\n",
    "1097 0.33 42\n",
    "80808 0.07 4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to arrays\n",
    "\n",
    "```python\n",
    "population_array = np.array(population)\n",
    "print(population_array) # Index info is lost\n",
    "[[ 322]\n",
    "[ 130]\n",
    "[40038]\n",
    "[45199]]\n",
    "\n",
    "unemployment_array = np.array(unemployment)\n",
    "print(population_array)\n",
    "[[ 1.10000000e-01 3.44470000e+04]\n",
    "[ 2.00000000e-02 4.80000000e+03]\n",
    "[ 3.30000000e-01 4.20000000e+01]\n",
    "[ 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating data as arrays\n",
    "\n",
    "```python\n",
    "print(np.concatenate([population_array, unemployment_array], axis=1))\n",
    "[[ 3.22000000e+02 1.10000000e-01 3.44470000e+04]\n",
    "[ 1.30000000e+02 2.00000000e-02 4.80000000e+03]\n",
    "[ 4.00380000e+04 3.30000000e-01 4.20000000e+01]\n",
    "[ 4.51990000e+04 7.00000000e-02 4.31000000e+03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins\n",
    "\n",
    "* Joining tables: Combining rows of multiple tables\n",
    "* Outer join\n",
    "    * Union of index sets (all labels, no repetition)\n",
    "    * Missing fields filled with NaN\n",
    "* Inner join\n",
    "    * Intersection of index sets (only common labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & inner join\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='inner')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "2860 45199 0.11 34447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation & outer join\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], axis=1, join='outer')\n",
    "\n",
    "2010 Census Population unemployment participants\n",
    "1097 NaN 0.33 42.0\n",
    "2860 45199.0 0.11 34447.0\n",
    "37660 40038.0 NaN NaN\n",
    "46167 NaN 0.02 4800.0\n",
    "57538 322.0 NaN NaN\n",
    "59916 130.0 NaN NaN\n",
    "80808 NaN 0.07 4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join on other axis\n",
    "\n",
    "```python\n",
    "pd.concat([population, unemployment], join='inner', axis=0)\n",
    "\n",
    "Empty DataFrame\n",
    "Columns: []\n",
    "Index: [2860, 46167, 1097, 80808, 57538, 59916, 37660, 2860]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling & concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Merging Data\n",
    "\n",
    "Here, you'll learn all about merging pandas DataFrames. You'll explore different techniques for merging, and learn about left joins, right joins, inner joins, and outer joins, as well as when to use which. You'll also learn about ordered merging, which is useful when you want to merge DataFrames whose columns have natural orderings, like date-time columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Merging DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population DataFrame\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "population = pd.read_csv('pa_zipcode_population.csv')\n",
    "print(population)\n",
    "Zipcode 2010 Census Population\n",
    "0 16855 282\n",
    "1 15681 5241\n",
    "2 18657 11985\n",
    "3 17307 5899\n",
    "4 15635 220\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities DataFrame\n",
    "\n",
    "```python\n",
    "cities = pd.read_csv('pa_zipcode_city.csv')\n",
    "print(cities)\n",
    "Zipcode City State\n",
    "0 17545 MANHEIM PA\n",
    "1 18455 PRESTON PARK PA\n",
    "2 17307 BIGLERVILLE PA\n",
    "3 15705 INDIANA PA\n",
    "4 16833 CURWENSVILLE PA\n",
    "5 16220 CROWN PA\n",
    "6 18618 HARVEYS LAKE PA\n",
    "7 16855 MINERAL SPRINGS PA\n",
    "8 16623 CASSVILLE PA\n",
    "9 15635 HANNASTOWN PA\n",
    "10 15681 SALTSBURG PA\n",
    "11 18657 TUNKHANNOCK PA\n",
    "12 15279 PITTSBURGH PA\n",
    "13 17231 LEMASTERS PA\n",
    "14 18821 GREAT BEND PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "```python\n",
    "pd.merge(population, cities)\n",
    "\n",
    "Zipcode 2010 Census Population City State\n",
    "0 16855 282 MINERAL SPRINGS PA\n",
    "1 15681 5241 SALTSBURG PA\n",
    "2 18657 11985 TUNKHANNOCK PA\n",
    "3 17307 5899 BIGLERVILLE PA\n",
    "4 15635 220 HANNASTOWN PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold)\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on='NOC')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(so_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'])\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counties DataFrame\n",
    "\n",
    "```python\n",
    "counties = pd.read_csv('pa_counties.csv')\n",
    "print(counties)\n",
    "CITY NAME COUNTY NAME\n",
    "0 SALTSBURG INDIANA\n",
    "1 MINERAL SPRINGS CLEARFIELD\n",
    "2 BIGLERVILLE ADAMS\n",
    "3 HANNASTOWN WESTMORELAND\n",
    "4 TUNKHANNOCK WYOMING\n",
    "\n",
    "print(cities.tail())\n",
    "Zipcode City State\n",
    "10 15681 SALTSBURG PA\n",
    "11 18657 TUNKHANNOCK PA\n",
    "12 15279 PITTSBURGH PA\n",
    "13 17231 LEMASTERS PA\n",
    "14 18821 GREAT BEND PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying columns to merge\n",
    "\n",
    "```python\n",
    "pd.merge(counties, cities, left_on='CITY NAME', right_on='City')\n",
    "\n",
    "CITY NAME COUNTY NAME Zipcode City State\n",
    "0 SALTSBURG INDIANA 15681 SALTSBURG PA\n",
    "1 MINERAL SPRINGS CLEARFIELD 16855 MINERAL SPRINGS PA\n",
    "2 BIGLERVILLE ADAMS 17307 BIGLERVILLE PA\n",
    "3 HANNASTOWN WESTMORELAND 15635 HANNASTOWN PA\n",
    "4 TUNKHANNOCK WYOMING 18657 TUNKHANNOCK PA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching left/right DataFrames\n",
    "\n",
    "```python\n",
    "pd.merge(cities, counties, left_on='City', right_on='CITY NAME')\n",
    "\n",
    "Zipcode City State CITY NAME COUNTY NAME\n",
    "0 17307 BIGLERVILLE PA BIGLERVILLE ADAMS\n",
    "1 16855 MINERAL SPRINGS PA MINERAL SPRINGS CLEARFIELD\n",
    "2 15635 HANNASTOWN PA HANNASTOWN WESTMORELAND\n",
    "3 15681 SALTSBURG PA SALTSBURG INDIANA\n",
    "4 18657 TUNKHANNOCK PA TUNKHANNOCK WYOMING\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging company DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on columns with non-matching labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medal DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = pd.read_csv(so_bronze_file)\n",
    "bronze.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(so_gold_file)\n",
    "gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_merge = pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='inner')\n",
    "so_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with left join\n",
    "\n",
    "* Keeps all rows of the left DF in the merged DF\n",
    "* For rows in the left DF with matches in the right DF:\n",
    "    * Non-joining columns of right DF are appended to left DF\n",
    "* For rows in the left DF with no matches in the right DF:\n",
    "    * Non-joining columns are filled with nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='left')\n",
    "\n",
    "  NOC Country             Total_bronze    Total_gold\n",
    "0 USA United States       1052.0          2088.0\n",
    "1 URS Soviet Union        584.0           838.0\n",
    "2 GBR United Kingdom      505.0           498.0\n",
    "3 FRA France              475.0           NaN\n",
    "4 GER Germany             454.0           407.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with right join\n",
    "\n",
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='right')\n",
    "\n",
    "  NOC Country          Total_bronze    Total_gold\n",
    "0 USA United States    1052.0          2088.0\n",
    "1 URS Soviet Union     584.0           838.0\n",
    "2 GBR United Kingdom   505.0           498.0\n",
    "3 GER Germany          454.0           407.0\n",
    "4 ITA Italy            NaN             460.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with outer join\n",
    "\n",
    "```python\n",
    "pd.merge(bronze, gold, on=['NOC', 'Country'], suffixes=['_bronze', '_gold'], how='outer')\n",
    "\n",
    "  NOC Country           Total_bronze   Total_gold\n",
    "0 USA United States     1052.0         2088.0\n",
    "1 URS Soviet Union      584.0          838.0\n",
    "2 GBR United Kingdom    505.0          498.0\n",
    "3 FRA France            475.0          NaN\n",
    "4 GER Germany           454.0          407.0\n",
    "5 ITA Italy             NaN            460.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population & unemployment data\n",
    "\n",
    "```python\n",
    "population = pd.read_csv('population_00.csv', index_col=0)\n",
    "unemployment = pd.read_csv('unemployment_00.csv', index_col=0)\n",
    "\n",
    "print(population)\n",
    "                2010 Census Population\n",
    "Zip Code ZCTA\n",
    "57538           322\n",
    "59916           130\n",
    "37660           40038\n",
    "2860            45199\n",
    "\n",
    "print(unemployment)\n",
    "         unemployment    participants\n",
    "Zip\n",
    "2860     0.11            34447\n",
    "46167    0.02            4800\n",
    "1097     0.33            42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='left')\n",
    "\n",
    "```python\n",
    "population.join(unemployment)\n",
    "\n",
    "                2010 Census Population unemployment participants\n",
    "Zip Code ZCTA\n",
    "57538           322                    NaN          NaN\n",
    "59916           130                    NaN          NaN\n",
    "37660           40038                  NaN          NaN\n",
    "2860            45199                  0.11         34447.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='right')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='right')\n",
    "\n",
    "        2010 Census Population unemployment participants\n",
    "Zip\n",
    "2860    45199.0                0.11         34447\n",
    "46167   NaN                    0.02         4800\n",
    "1097    NaN                    0.33         42\n",
    "80808   NaN                    0.07         4310\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='inner')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='inner')\n",
    "\n",
    "        2010 Census Population   unemployment   participants\n",
    "2860                     45199   0.11           34447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using .join(how='outer')\n",
    "\n",
    "```python\n",
    "population.join(unemployment, how='outer')\n",
    "\n",
    "        2010 Census Population    unemployment participants\n",
    "1097                   NaN        0.33         42.0\n",
    "2860                   45199.0    0.11         34447.0\n",
    "37660                  40038.0    NaN          NaN\n",
    "46167                  NaN        0.02         4800.0\n",
    "57538                  322.0      NaN          NaN\n",
    "59916                  130.0      NaN          NaN\n",
    "80808                  NaN        0.07         4310.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which should you use?\n",
    "\n",
    "* df1.append(df2): stacking vertically\n",
    "* pd.concat([df1, df2]):\n",
    "    * stacking many horizontally or vertically\n",
    "    * simple inner/outer joins on Indexes\n",
    "* df1.join(df2): inner/outer/left/right joins on Indexes\n",
    "* pd.merge([df1, df2]): many joins on multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a joining strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left & right merging on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging DataFrames with outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software & hardware sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software = pd.read_csv(sales_feb_software_file, parse_dates=['Date']).sort_values('Date')\n",
    "software.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = pd.read_csv(sales_feb_hardware_file, parse_dates=['Date']).sort_values('Date')\n",
    "hardware.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software)\n",
    "sales_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting merge(how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merge = pd.merge(hardware, software, how='outer').sort_values('Date')\n",
    "sales_merge.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software)\n",
    "sales_merged.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using on & suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_merged = pd.merge_ordered(hardware, software, on=['Date', 'Company'], suffixes=['_hardware', '_software'])\n",
    "sales_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = data / 'stocks'\n",
    "sp500 = stocks_dir / 'SP500.csv'\n",
    "aapl = stocks_dir/ 'AAPL.csv'\n",
    "csco = stocks_dir/ 'CSCO.csv'\n",
    "amzn = stocks_dir/ 'AMZN.csv'\n",
    "msft = stocks_dir/ 'MSFT.csv'\n",
    "ibm = stocks_dir/ 'IBM.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df = pd.read_csv(sp500, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "aapl_df = pd.read_csv(aapl, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "csco_df = pd.read_csv(csco, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "amzn_df = pd.read_csv(amzn, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "msft_df = pd.read_csv(msft, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])\n",
    "ibm_df = pd.read_csv(ibm, usecols=['Date', 'Close'], parse_dates=['Date'], index_col=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df.rename(columns={'Close': 'S&P'}, inplace=True)\n",
    "aapl_df.rename(columns={'Close': 'AAPL'}, inplace=True)\n",
    "csco_df.rename(columns={'Close': 'CSCO'}, inplace=True)\n",
    "amzn_df.rename(columns={'Close': 'AMZN'}, inplace=True)\n",
    "msft_df.rename(columns={'Close': 'MSFT'}, inplace=True)\n",
    "ibm_df.rename(columns={'Close': 'IBM'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.concat([sp500_df, aapl_df, csco_df, amzn_df, msft_df, ibm_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.to_csv(stocks_dir / 'stocks.csv', index=True, index_label='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = pd.read_csv(gdp_usa_file, parse_dates=['DATE'])\n",
    "gdp.sort_values(by=['DATE'], ascending=False, inplace=True)\n",
    "gdp.reset_index(inplace=True, drop=True)\n",
    "gdp.rename(columns={'VALUE': 'GDP', 'DATE': 'Date'}, inplace=True)\n",
    "gdp.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_2000_2015 = gdp[(gdp['Date'].dt.year >= 2000) & (gdp['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.reset_index(inplace=True)\n",
    "stocks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_2000_2015 = stocks[(stocks['Date'].dt.year >= 2000) & (stocks['Date'].dt.year <= 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordered merge with ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_df = pd.merge_ordered(stocks_2000_2015, gdp_2000_2015, on='Date', fill_method='ffill')\n",
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using merge_asof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Summer Olympics\n",
    "\n",
    "To cement your new skills, you'll apply them by working on an in-depth study involving Olympic medal data. The analysis involves integrating your multi-DataFrame skills from this course and also skills you've gained in previous pandas courses. This is a rich dataset that will allow you to fully leverage your pandas data manipulation skills. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medals in the Summer Olympics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_ioc_codes_file).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_editions_file, sep='\\t').head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summer_1896.csv, summer_1900.csv, …, summer_2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(so_all_medalists_file, sep='\\t', header=4).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: loading & merging files\n",
    "\n",
    "* pd.read_csv() (& its many options)\n",
    "* Looping over files, e.g.,\n",
    "    * [pd.read_csv(f) for f in glob('*.csv')]\n",
    "* Concatenating & appending, e.g.,\n",
    "    * pd.concat([df1, df2], axis=0)\n",
    "    * df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Olympic edition DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading IOC codes DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building medals DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a pivot table\n",
    "\n",
    "* Apply DataFrame pivot_table() method\n",
    "    * index: column to use as index of pivot table\n",
    "    * values: column(s) to aggregate\n",
    "    * aggfunc: function to apply for aggregation\n",
    "    * columns: categories as columns of pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting medals by country/edition in a pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing fraction of medals per Olympic edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing percentage change in fraction of medals won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building hosts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging to compute influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting influence of host country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
