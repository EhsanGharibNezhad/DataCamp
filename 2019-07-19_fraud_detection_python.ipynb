{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fraud Detection in Python**\n",
    "\n",
    "**Course Description**\n",
    "\n",
    "A typical organization loses an estimated 5% of its yearly revenue to fraud. In this course, learn to fight fraud by using data. Apply supervised learning algorithms to detect fraudulent behavior based upon past fraud, and use unsupervised learning methods to discover new types of fraud activities. \n",
    "\n",
    "Fraudulent transactions are rare compared to the norm.  As such, learn to properly classify imbalanced datasets.\n",
    "\n",
    "The course provides technical and theoretical insights and demonstrates how to implement fraud detection models. Finally, get tips and advice from real-life experience to help prevent common mistakes in fraud analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas Configuration Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 300)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Files Location**\n",
    "\n",
    "* Most data files for the exercises can be found on the [course site](https://www.datacamp.com/courses/fraud-detection-in-python)\n",
    "    * [Chapter 1](https://assets.datacamp.com/production/repositories/2162/datasets/cc3a36b722c0806e4a7df2634e345975a0724958/chapter_1.zip)\n",
    "    * [Chapter 2](https://assets.datacamp.com/production/repositories/2162/datasets/4fb6199be9b89626dcd6b36c235cbf60cf4c1631/chapter_2.zip)\n",
    "    * [Chapter 3](https://assets.datacamp.com/production/repositories/2162/datasets/08cfcd4158b3a758e72e9bd077a9e44fec9f773b/chapter_3.zip)\n",
    "    * [Chapter 4](https://assets.datacamp.com/production/repositories/2162/datasets/94f2356652dc9ea8f0654b5e9c29645115b6e77f/chapter_4.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data File Objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd() / 'data' / 'fraud_detection'\n",
    "\n",
    "ch1 = data / 'chapter_1'\n",
    "cc1_file = ch1 / 'creditcard_sampledata.csv'\n",
    "cc3_file = ch1 / 'creditcard_sampledata_3.csv'\n",
    "\n",
    "ch2 = data / 'chapter_2'\n",
    "cc2_file = ch2 / 'creditcard_sampledata_2.csv'\n",
    "\n",
    "ch3 = data / 'chapter_3'\n",
    "banksim_file = ch3 / 'banksim.csv'\n",
    "banksim_adj_file = ch3 / 'banksim_adj.csv'\n",
    "db_full_file = ch3 / 'db_full.pickle'\n",
    "labels_file = ch3 / 'labels.pickle'\n",
    "labels_full_file = ch3 / 'labels_full.pickle'\n",
    "x_scaled_file = ch3 / 'x_scaled.pickle'\n",
    "x_scaled_full_file = ch3 / 'x_scaled_full.pickle'\n",
    "\n",
    "ch4 = data / 'chapter_4'\n",
    "enron_emails_clean_file = ch4 / 'enron_emails_clean.csv'\n",
    "cleantext_file = ch4 / 'cleantext.pickle'\n",
    "corpus_file = ch4 / 'corpus.pickle'\n",
    "dict_file = ch4 / 'dict.pickle'\n",
    "ldamodel_file = ch4 / 'ldamodel.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and preparing your data\n",
    "\n",
    "Learn about the typical challenges associated with fraud detection. Learn how to resample data in a smart way, and tackle problems with imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to fraud detection\n",
    "\n",
    "* Types:\n",
    "    * Insurance\n",
    "    * Credit card\n",
    "    * Identity theft\n",
    "    * Money laundering\n",
    "    * Tax evasion\n",
    "    * Healthcare\n",
    "    * Product warranty\n",
    "* e-commerce businesses must continuously assess the legitimacy of client transactions\n",
    "* Detecting fraud is challenging:\n",
    "    * Uncommon; < 0.01% of transactions\n",
    "    * Attempts are made to conceal fraud\n",
    "    * Behavior evolves\n",
    "    * Fraudulent activities perpetrated by networks - organized crime\n",
    "* Fraud detection requires training an algorithm to identify concealed observations from any normal observations\n",
    "* Fraud analytics teams:\n",
    "    * Often use rules based systems, based on manually set thresholds and experience\n",
    "    * Check the news\n",
    "    * Receive external lists of fraudulent accounts and names\n",
    "        * suspicious names or track an external hit list from police to reference check against the client base\n",
    "    * Sometimes use machine learning algorithms to detect fraud or suspicious behavior\n",
    "        * Existing sources can be used as inputs into the ML model\n",
    "        * Verify the veracity of rules based labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the fraud to non-fraud ratio\n",
    "\n",
    "In this chapter, you will work on `creditcard_sampledata.csv`, a dataset containing credit card transactions data. Fraud occurrences are fortunately an **extreme minority** in these transactions.\n",
    "\n",
    "However, Machine Learning algorithms usually work best when the different classes contained in the dataset are more or less equally present. If there are few cases of fraud, then there's little data to learn how to identify them. This is known as **class imbalance**, and it's one of the main challenges of fraud detection.\n",
    "\n",
    "Let's explore this dataset, and observe this class imbalance problem.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* `import pandas as pd`, read the credit card data in and assign it to `df`. This has been done for you.\n",
    "* Use `.info()` to print information about `df`.\n",
    "* Use `.value_counts()` to get the count of fraudulent and non-fraudulent transactions in the `'Class'` column. Assign the result to `occ`.\n",
    "* Get the ratio of fraudulent transactions over the total number of transactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cc3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the features available in your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = df['Class'].value_counts()\n",
    "occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the ratio of fraud cases\n",
    "ratio_cases = occ/len(df.index)\n",
    "print(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ratio of fraudulent transactions is very low. This is a case of class imbalance problem, and you're going to learn how to deal with this in the next exercises.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "From the previous exercise we know that the ratio of fraud to non-fraud observations is very low. You can do something about that, for example by **re-sampling** our data, which is explained in the next video.\n",
    "\n",
    "In this exercise, you'll look at the data and **visualize the fraud to non-fraud ratio**. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it.\n",
    "\n",
    "Moreover, when talking to your colleagues, a picture often makes it very clear that we're dealing with heavily imbalanced data. Let's create a plot to visualize the ratio fraud to non-fraud data points on the dataset `df`.\n",
    "\n",
    "The function `prep_data()` is already loaded in your workspace, as well as `matplotlib.pyplot as plt`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Define the `plot_data(X, y)` function, that will nicely plot the given feature set `X` with labels `y` in a scatter plot. This has been done for you.\n",
    "* Use the function `prep_data()` on your dataset `df` to create feature set `X` and labels `y`.\n",
    "* Run the function `plot_data()` on your newly obtained `X` and `y` to visualize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df: pd.DataFrame) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame into two variable\n",
    "    X: data columns (V1 - V28)\n",
    "    y: lable column\n",
    "    \"\"\"\n",
    "    X = df.iloc[:, 2:30].values\n",
    "    y = df.Class.values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a scatter plot of our data and labels\n",
    "def plot_data(X: np.ndarray, y: np.ndarray):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y from the prep_data function \n",
    "X, y = prep_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our data by running our plot data function on X and y\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By visualizing the data, you can immediately see how our fraud cases are scattered over our data, and how few cases we have. A picture often makes the imbalance problem clear. In the next exercises we'll visually explore how to improve our fraud to non-fraud balance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproduced using the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.V2[df.Class == 0], df.V3[df.Class == 0], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "plt.scatter(df.V2[df.Class == 1], df.V3[df.Class == 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase successful detections with data resampling\n",
    "\n",
    "* resampling can help model performance in cases of imbalanced data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling\n",
    "\n",
    "* ![undersampling](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/undersampling.JPG)\n",
    "* Undersampling the majority class (non-fraud cases)\n",
    "    * Straightforward method to adjust imbalanced data\n",
    "    * Take random draws from the non-fraud observations, to match the occurences of fraud observations (as shown in the picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling\n",
    "\n",
    "* ![oversampling](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/oversampling.JPG)\n",
    "* Oversampling the minority class (fraud cases)\n",
    "    * Take random draws from the fraud cases and copy those observations to increase the amount of fraud samples\n",
    "* Both methods lead to having a balance between fraud and non-fraud cases\n",
    "* Drawbacks\n",
    "    * with random undersampling, a lot of information is thrown away\n",
    "    * with oversampling, the model will be trained on a lot of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement resampling methods using Python imblean module\n",
    "\n",
    "* compatible with scikit-learn\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "method = RandomOverSampler()\n",
    "X_resampled, y_resampled =  method.fit_sample(X, y)\n",
    "\n",
    "compare_plots(X_resampled, y_resampled, X, y)\n",
    "```\n",
    "\n",
    "![oversampling plot](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/oversampling_plot.JPG)\n",
    "* The darker blue points reflect there are more identical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "\n",
    "* ![smote](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/smote.JPG)\n",
    "* Synthetic minority Oversampling Technique (SMOTE)\n",
    "    * [Resampling strategies for Imbalanced Data Sets](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)\n",
    "    * Another way of adjusting the imbalance by oversampling minority observations\n",
    "    * SMOTE uses characteristics of nearest neighbors of fraud cases to create new synthetic fraud cases\n",
    "        * avoids duplicating observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the best resampling method is situational\n",
    "\n",
    "* Random Undersampling (RUS):\n",
    "    * If there is a lot of data and many minority cases, then undersampling may be computationally more convenient\n",
    "        * In most cases, throwing away data is not desirable\n",
    "* Random Oversampling (ROS):\n",
    "    * Straightforward\n",
    "    * Training the model on many duplicates\n",
    "* SMOTE:\n",
    "    * more sophisticated\n",
    "    * realistic data set\n",
    "    * training on synthetic data\n",
    "    * only works well if the minority case features are similar\n",
    "        * **if fraud is spread through the data and not distinct, using nearest neighbors to create more fraud cases, introduces noise into the data, as the nearest neighbors might not be fraud cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use resmapling methods\n",
    "\n",
    "* Use resampling methods on the training set, not on the test set\n",
    "* The goal is to produce a better model by providing balanced data\n",
    "    * The goal is not to predict the synthetic samples\n",
    "* Test data should be free of duplicates and synthetic data\n",
    "* Only test the model on real data\n",
    "    * First, spit the data into train and test sets\n",
    "    \n",
    "```python\n",
    "# Define resampling method and split into train and test\n",
    "method = SMOTE(kind='borderline1')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "# Apply resampling to the training data only\n",
    "X_resampled, y_resampled = method.fit_sample(X_train, y_train)\n",
    "\n",
    "# Continue fitting the model and obtain predictions\n",
    "model = LogisticRegression()\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Get model performance metrics\n",
    "predicted = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling methods for imbalanced data\n",
    "\n",
    "Which of these methods takes a random subsample of your majority class to account for class \"imbalancedness\"?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "* ~~Random Over Sampling (ROS)~~\n",
    "* **Random Under Sampling (RUS)**\n",
    "* ~~Synthetic Minority Over-sampling Technique (SMOTE)~~\n",
    "* ~~None of the above~~\n",
    "\n",
    "**By using ROS and SMOTE you add more examples to the minority class. RUS adjusts the balance of your data by reducing the majority class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "In this exercise, you're going to re-balance our data using the **Synthetic Minority Over-sampling Technique** (SMOTE). Unlike ROS, SMOTE does not create exact copies of observations, but **creates new, synthetic, samples** that are quite similar to the existing observations in the minority class. SMOTE is therefore slightly more sophisticated than just copying observations, so let's apply SMOTE to our credit card data. The dataset `df` is available and the packages you need for SMOTE are imported. In the following exercise, you'll visualize the result and compare it to the original data, such that you can see the effect of applying SMOTE very clearly.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Use the `prep_data` function on `df` to create features `X` and labels `y`.\n",
    "* Define the resampling method as SMOTE of the regular kind, under the variable `method`.\n",
    "* Use `.fit_sample()` on the original `X` and `y` to obtain newly resampled data.\n",
    "* Plot the resampled data using the `plot_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prep_data function\n",
    "X, y = prep_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X shape: {X.shape}\\ny shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resampling method\n",
    "method = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled data\n",
    "plot_data(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The minority class is now much more prominently visible in our data. To see the results of SMOTE even better, we'll compare it to the original data in the next exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SMOTE to original data\n",
    "\n",
    "In the last exercise, you saw that using SMOTE suddenly gives us more observations of the minority class. Let's compare those results to our original data, to get a good feeling for what has actually happened. Let's have a look at the value counts again of our old and new data, and let's plot the two scatter plots of the data side by side. You'll use the function compare_plot() for that that, which takes the following arguments: `X`, `y`, `X_resampled`, `y_resampled`, `method=''`. The function plots your original data in a scatter plot, along with the resampled side by side.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Print the value counts of our original labels, `y`. Be mindful that `y` is currently a Numpy array, so in order to use value counts, we'll assign `y` back as a pandas Series object.\n",
    "* Repeat the step and print the value counts on `y_resampled`. This shows you how the balance between the two classes has changed with SMOTE.\n",
    "* Use the `compare_plot()` function called on our original data as well our resampled data to see the scatterplots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pd.Series(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pd.Series(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def compare_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(X: np.ndarray, y: np.ndarray, X_resampled: np.ndarray, y_resampled: np.ndarray, method: str):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.title('Original Set')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "    plt.title(method)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_plot(X, y, X_resampled, y_resampled, method='SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It should by now be clear that SMOTE has balanced our data completely, and that the minority class is now equal in size to the majority class. Visualizing the data shows the effect on the data very clearly. The next exercise will demonstrate multiple ways to implement SMOTE and that each method will have a slightly different effect.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud detection algorithms in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules Based Systems\n",
    "\n",
    "* ![rules based](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/rules_based.JPG)\n",
    "* Might block transactions from risky zip codes\n",
    "* Block transactions from cards used too frequently (e.g. last 30 minutes)\n",
    "* Can catch fraud, but also generates false alarms (false positive)\n",
    "* Limitations:\n",
    "    * Fixed threshold per rule and it's difficult to determine the threshold; they don't adapt over time\n",
    "    * Limited to yes / no outcomes, whereas ML yields a probability\n",
    "        * probability allows for fine-tuning the outcomes (i.e. rate of occurences of false positives and false negatives)\n",
    "    * Fails to capture interaction between features\n",
    "        * Ex. Size of the transaction only matters in combination to the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Based Systems\n",
    "\n",
    "* Adapt to the data, thus can change over time\n",
    "* Uses all the data combined, rather than a threshold per feature\n",
    "* Produces a probability, rather than a binary score\n",
    "* Typically have better performance and can be combined with rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: split the features and labels into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define which model to use\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fit the model to the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Obtain model predictions from the test data\n",
    "y_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compare y_test to predictions and obtain performance metrics (r^2 score)\n",
    "r2_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the traditional method of fraud detection\n",
    "\n",
    "In this exercise you're going to try finding fraud cases in our credit card dataset the *\"old way\"*. First you'll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams.\n",
    "\n",
    "Statistical thresholds are often determined by looking at the **mean** values of observations. Let's start this exercise by checking whether feature **means differ between fraud and non-fraud cases**. Then, you'll use that information to create common sense thresholds. Finally, you'll check how well this performs in fraud detection.\n",
    "\n",
    "`pandas` has already been imported as `pd`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Use `groupby()` to group `df` on `Class` and obtain the mean of the features.\n",
    "* Create the condition `V1` smaller than -3, and `V3` smaller than -5 as a condition to flag fraud cases.\n",
    "* As a measure of performance, use the `crosstab` function from `pandas` to compare our flagged fraud cases to actual fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flag_as_fraud'] = np.where(np.logical_and(df.V1 < -3, df.V3 < -5), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With this rule, 22 out of 50 fraud cases are detected, 28 are not detected, and 16 false positives are identified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML classification to catch fraud\n",
    "\n",
    "In this exercise you'll see what happens when you use a simple machine learning model on our credit card data instead.\n",
    "\n",
    "Do you think you can beat those results? Remember, you've predicted *22 out of 50* fraud cases, and had *16 false positives*.\n",
    "\n",
    "So with that in mind, let's implement a **Logistic Regression** model. If you have taken the class on supervised learning in Python, you should be familiar with this model. If not, you might want to refresh that at this point. But don't worry, you'll be guided through the structure of the machine learning model.\n",
    "\n",
    "The `X` and `y` variables are available in your workspace.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Split `X` and `y` into training and test data, keeping 30% of the data for testing.\n",
    "* Fit your model to your training data.\n",
    "* Obtain the model predicted labels by running `model.predict` on `X_test`.\n",
    "* Obtain a classification comparing `y_test` with `predicted`, and use the given confusion matrix to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model to our data\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain model predictions\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classifcation report and confusion matrix\n",
    "print('Classification report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you think these results are better than the rules based model? We are getting far fewer false positives, so that's an improvement. Also, we're catching a higher percentage of fraud cases, so that is also better than before. Do you understand why we have fewer observations to look at in the confusion matrix? Remember we are using only our test data to calculate the model results on. We're comparing the crosstab on the full dataset from the last exercise, with a confusion matrix of only 30% of the total dataset, so that's where that difference comes from. In the next chapter, we'll dive deeper into understanding these model performance metrics. Let's now explore whether we can improve the prediction results even further with resampling methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with SMOTE\n",
    "\n",
    "In this exercise, you're going to take the Logistic Regression model from the previous exercise, and combine that with a **SMOTE resampling method**. We'll show you how to do that efficiently by using a pipeline that combines the resampling method with the model in one go. First, you need to define the pipeline that you're going to use.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import the `Pipeline` module from `imblearn`, this has been done for you.\n",
    "* Then define what you want to put into the pipeline, assign the `SMOTE` method with `borderline2` to `resampling`, and assign `LogisticRegression()` to the `model`.\n",
    "* Combine two steps in the `Pipeline()` function. You need to state you want to combine `resampling` with the `model` in the respective place in the argument. I show you how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which resampling method and which ML model to use in the pipeline\n",
    "resampling = SMOTE(kind='borderline2')\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "Now that you have our pipeline defined, aka **combining a logistic regression with a SMOTE method**, let's run it on the data. You can treat the pipeline as if it were a **single machine learning model**. Our data X and y are already defined, and the pipeline is defined in the previous exercise. Are you curious to find out what the model results are? Let's give it a try!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Split the data 'X'and 'y' into the training and test set. Set aside 30% of the data for a test set, and set the `random_state` to zero.\n",
    "* Fit your pipeline onto your training data and obtain the predictions by running the `pipeline.predict()` function on our `X_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train) \n",
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results from the classification report and confusion matrix \n",
    "print('Classifcation report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, the SMOTE slightly improves our results. We now manage to find all cases of fraud, but we have a slightly higher number of false positives, albeit only 7 cases. Remember, resampling doesn't necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren't necessarily also fraud cases, so the synthetic samples might 'confuse' the model slightly. In the next chapters, we'll learn how to also adjust our machine learning models to better detect the minority fraud cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection using labeled data\n",
    "\n",
    "Learn how to flag fraudulent transactions with supervised learning. Use classifiers, adjust and compare them to find the most efficient fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review classification methods\n",
    "\n",
    "* Classification:\n",
    "    * The problem of identifying to which class a new observation belongs, on the basis of a training set of data containing observations whose class is known\n",
    "    * Goal: use known fraud cases to train a model to recognize new cases\n",
    "    * Classes are sometimes called targets, labels or categories\n",
    "    * Spam detection in email service providers can be identified as a classification problem\n",
    "        * Binary classification since there are only 2 classes, spam and not spam\n",
    "    * Fraud detection is also a binary classification prpoblem\n",
    "    * Patient diagnosis\n",
    "    * Classification problems normall have categorical output like yes/no, 1/0 or True/False\n",
    "    * Variable to predict: $$y\\in0,1$$\n",
    "        * 0: negative calss ('majority' normal cases)\n",
    "        * 1: positive class ('minority' fraud cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "* Logistic Regression is one of the most used ML algorithms in binary classification\n",
    "* ![logistic regression](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/logistic_regression.JPG)\n",
    "* Can be adjusted reasonably well to work on imbalanced data...useful for fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "\n",
    "* ![neural network](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/neural_network.JPG)\n",
    "* Can be used as classifiers for fraud detection\n",
    "* Capable of fitting highly non-linear models to the data\n",
    "* More complex to implement than other classifiers - not demonstrated here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "* ![decision tree](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/decision_tree.JPG)\n",
    "* Commonly used for fraud detection\n",
    "* Transparent results, easily interpreted by analysts\n",
    "* Decision trees are prone to overfit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "* ![random forest](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/random_forest.JPG)\n",
    "* **Random Forests are a more robust option than a single decision tree**\n",
    "    * Construct a multitude of decision trees when training the model and outputting the class that is the mode or mean predicted class of the individual trees\n",
    "    * A random forest consists of a collection of trees on a random subset of features\n",
    "    * Final predictions are the combined results of those trees\n",
    "    * Random forests can handle complex data and are not prone to overfit\n",
    "    * They are interpretable by looking at feature importance, and can be adjusted to work well on highly imbalanced data\n",
    "    * Their drawback is they're computationally complex\n",
    "    * Very popular for fraud detection\n",
    "    * A Random Forest model will be optimized in the exercises\n",
    "    \n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "print(f'Accuracy Score:\\n{accuracy_score(y_test, predicted)}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural hit rate\n",
    "\n",
    "In this exercise, you'll again use credit card transaction data. The features and labels are similar to the data in the previous chapter, and the **data is heavily imbalanced**. We've given you features `X` and labels `y` to work with already, which are both numpy arrays.\n",
    "\n",
    "First you need to explore how prevalent fraud is in the dataset, to understand what the **\"natural accuracy\"** is, if we were to predict everything as non-fraud. It's is important to understand which level of \"accuracy\" you need to \"beat\" in order to get a **better prediction than by doing nothing**. In the following exercises, you'll create our first random forest classifier for fraud detection. That will serve as the **\"baseline\"** model that you're going to try to improve in the upcoming exercises.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Count the total number of observations by taking the length of your labels `y`.\n",
    "* Count the non-fraud cases in our data by using list comprehension on `y`; remember `y` is a NumPy array so `.value_counts()` cannot be used in this case.\n",
    "* Calculate the natural accuracy by dividing the non-fraud cases over the total observations.\n",
    "* Print the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(cc2_file)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prep_data(df2)\n",
    "print(f'X shape: {X.shape}\\ny shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of observations from the length of y\n",
    "total_obs = len(y)\n",
    "total_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of non-fraudulent observations \n",
    "non_fraud = [i for i in y if i == 0]\n",
    "count_non_fraud = non_fraud.count(0)\n",
    "count_non_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = count_non_fraud/total_obs * 100\n",
    "print(f'{percentage:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tells us that by doing nothing, we would be correct in 95.9% of the cases. So now you understand, that if we get an accuracy of less than this number, our model does not actually add any value in predicting how many cases are correct. Let's see how a random forest does in predicting fraud in our data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - part 1\n",
    "\n",
    "Let's now create a first **random forest classifier** for fraud detection. Hopefully you can do better than the baseline accuracy you've just calculated, which was roughly **96%**. This model will serve as the **\"baseline\" model** that you're going to try to improve in the upcoming exercises. Let's start first with **splitting the data into a test and training set**, and **defining the Random Forest model**. The data available are features `X` and labels `y`.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import the random forest classifier from `sklearn`.\n",
    "* Split your features `X` and labels `y` into a training and test set. Set aside a test set of 30%.\n",
    "* Assign the random forest classifier to `model` and keep `random_state` at 5. We need to set a random state here in order to be able to compare results across different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model as the random forest\n",
    "model = RandomForestClassifier(random_state=5, n_estimators=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - part 2\n",
    "\n",
    "Let's see how our Random Forest model performs **without doing anything special to it**. The `model` from the previous exercise is available, and you've already split your data in `X_train, y_train, X_test, y_test`.\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "* Fit the earlier defined `model` to our training data and obtain predictions by getting the model predictions on `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions from the test data \n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 2/3**\n",
    "\n",
    "* Obtain and print the accuracy score by comparing the actual labels `y_test` with our predicted labels `predicted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy Score:\\n{accuracy_score(y_test, predicted):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 3/3**\n",
    "\n",
    "What is a benefit of using Random Forests versus Decision Trees?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "* ~~Random Forests always have a higher accuracy than Decision Trees.~~\n",
    "* **Random Forests do not tend to overfit, whereas Decision Trees do.**\n",
    "* ~~Random Forests are computationally more efficient than Decision Trees.~~\n",
    "* ~~You can obtain \"feature importance\" from Random Forest, which makes it more transparent.~~\n",
    "\n",
    "**Random Forest prevents overfitting most of the time, by creating random subsets of the features and building smaller trees using these subsets. Afterwards, it combines the subtrees of subsamples of features, so it does not tend to overfit to your entire feature set the way \"deep\" Decisions Trees do.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomance evaluation\n",
    "\n",
    "* Performance metrics for fraud detection models\n",
    "* There are other performace metrics that are more informative and reliable than accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "![accuracy](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/accuracy.JPG)\n",
    "* Accuracy isn't a reliable performance metric when working with highly imbalanced data (such as fraud detection)\n",
    "* By doing nothing, aka predicting everything is the majority class (right image), a higher accuracy is obtained than by trying to build a predictive model (left image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "![advanced confusion matrix](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/confusion_matrix_advanced.JPG)\n",
    "![confusion matrix](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/confusion_matrix.JPG)\n",
    "* [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "* False Positives (FP) / False Negatives (FN)\n",
    "    * FN: predicts the person is not pregnant, but actually is\n",
    "        * Cases of fraud not caught by the model\n",
    "    * FP: predicts the person is pregnant, but actually is not\n",
    "        * Cases of 'false alarm'\n",
    "    * the business case determines whether FN or FP cases are more important\n",
    "        * a credit card company might want to catch as much fraud as possible and reduce false negatives, as fraudulent transactions can be incredibly costly\n",
    "            * a false alarm just means a transaction is blocked\n",
    "        * an insurance company can't handle many false alarms, as it means getting a team of investigators involved for each positive prediction\n",
    "        \n",
    "* True Positives / True Negatives are the cases predicted correctly (e.g. fraud / non-fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Recall\n",
    "\n",
    "* Credit card company wants to optimize for recall\n",
    "* Insurance company wants to optimize for precision\n",
    "* Precision:\n",
    "    * $$Precision=\\frac{\\#\\space True\\space Positives}{\\#\\space True\\space Positives+\\#\\space False\\space Positives}$$\n",
    "    * Fraction of actual fraud cases out of all predicted fraud cases\n",
    "        * true positives relative to the sum of true positives and false positives\n",
    "* Recall:\n",
    "    * $$Recall=\\frac{\\#\\space True\\space Positives}{\\#\\space True\\space Positives+\\#\\space False\\space Negatives}$$\n",
    "    * Fraction of predicted fraud cases out of all actual fraud cases\n",
    "        * true positives relative to the sum of true positives and false negative\n",
    "* Precision and recall are typically inversely related\n",
    "    * As precision increases, recall falls and vice-versa\n",
    "    * ![precision recall inverse relation](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/precision_recall_inverse.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Score\n",
    "\n",
    "* Weighs both precision and recall into on measure\n",
    "\n",
    "\\begin{align}\n",
    "F-measure = \\frac{2\\times{Precision}\\times{Recall}}{Precision\\times{Recall}} \\\\ \n",
    "\\\\\n",
    "= \\frac{2\\times{TP}}{2\\times{TP}+FP+FN}\n",
    "\\end{align}\n",
    "\n",
    "* is a performance metric that takes into account a balance between Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining performance metrics from sklean\n",
    "\n",
    "```python\n",
    "# import the methods\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Calculate average precision and the PR curve\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "\n",
    "# Obtain precision and recall\n",
    "precision, recall = precision_recall_curve(y_test, predicted)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic (ROC) curve to compare algorithms\n",
    "\n",
    "* Created by plotting the true positive rate against the false positive rate at various threshold settings\n",
    "* ![roc curve](https://raw.githubusercontent.com/trenton3983/DataCamp/master/Images/fraud_detection/roc_curve.JPG)\n",
    "* Useful for comparing performance of different algorithms\n",
    "\n",
    "```python\n",
    "# Obtain model probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print ROC_AUC score using probabilities\n",
    "print(metrics.roc_auc_score(y_test, probs[:, 1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix and classification report\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Obtain predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Print classification report using predictions\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics for the RF model\n",
    "\n",
    "In the previous exercises you obtained an accuracy score for your random forest model. This time, we know **accuracy can be misleading** in the case of fraud detection. With highly imbalanced fraud data, the AUROC curve is a more reliable performance metric, used to compare different classifiers. Moreover, the *classification report* tells you about the precision and recall of your model, whilst the *confusion matrix* actually shows how many fraud cases you can predict correctly. So let's get these performance metrics.\n",
    "\n",
    "You'll continue working on the same random forest model from the previous exercise. Your model, defined as `model = RandomForestClassifier(random_state=5)` has been fitted to your training data already, and `X_train, y_train, X_test, y_test` are available.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Import the classification report, confusion matrix and ROC score from `sklearn.metrics`.\n",
    "* Get the binary predictions from your trained random forest `model`.\n",
    "* Get the predicted probabilities by running the `predict_proba()` function.\n",
    "* Obtain classification report and confusion matrix by comparing `y_test` with `predicted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predictions from our random forest model \n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print('ROC Score:')\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predicted))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have now obtained more meaningful performance metrics that tell us how well the model performs, given the highly imbalanced data that you're working with. The model predicts 76 cases of fraud, out of which 73 are actual fraud. You have only 3 false positives. This is really good, and as a result you have a very high precision score. You do however, miss 18 cases of actual fraud. Recall is therefore not as good as precision.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Precision vs. Recall Curve\n",
    "\n",
    "You can also plot a **Precision-Recall curve**, to investigate the trade-off between the two in your model. In this curve **Precision and Recall are inversely related**; as Precision increases, Recall falls and vice-versa. A balance between these two needs to be achieved in your model, otherwise you might end up with many false positives, or not enough actual fraud cases caught. To achieve this and to compare performance, the precision-recall curves come in handy.\n",
    "\n",
    "Your Random Forest Classifier is available as `model`, and the predictions as `predicted`. You can simply obtain the average precision score and the PR curve from the sklearn package. The function `plot_pr_curve()` plots the results for you. Let's give it a try.\n",
    "\n",
    "**Instructions 1/3**\n",
    "\n",
    "* Calculate the average precision by running the function on the actual labels `y_test` and your predicted labels `predicted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average precision and the PR curve\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 2/3**\n",
    "\n",
    "* Run the `precision_recall_curve()` function on the same arguments `y_test` and `predicted` and plot the curve (this last thing has been done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain precision and recall \n",
    "precision, recall, _ = precision_recall_curve(y_test, predicted)\n",
    "print(f'Precision: {precision}\\nRecall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def plot_pr_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision, average_precision):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "    \"\"\"\n",
    "    from inspect import signature\n",
    "    plt.figure()\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(f'2-class Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the recall precision tradeoff\n",
    "plot_pr_curve(recall, precision, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions 3/3**\n",
    "\n",
    "What's the benefit of the performance metric ROC curve (AUROC) versus Precision and Recall?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "* **The AUROC answers the question: \"How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?\" but precision and recall don't.**\n",
    "* ~~The AUROC answers the question: \"How meaningful is a positive result from my classifier given the baseline probabilities of my problem?\" but precision and recall don't.~~\n",
    "* ~~Precision and Recall are not informative when the data is imbalanced.~~\n",
    "* ~~The AUROC curve allows you to visualize classifier performance and with Precision and Recall you cannot.~~\n",
    "\n",
    "**The ROC curve plots the true positives vs. false positives , for a classifier, as its discrimination threshold is varied. Since, a random method describes a horizontal curve through the unit interval, it has an AUC of 0.5. Minimally, classifiers should perform better than this, and the extent to which they score higher than one another (meaning the area under the ROC curve is larger), they have better expected performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the algorithm weights\n",
    "\n",
    "* Adjust model parameter to optimize for fraud detection.\n",
    "* When training a model, try different options and settings to get the best recall-precision trade-off\n",
    "* sklearn has two simple options to tweak the model for heavily imbalanced data\n",
    "    * `class_weight`:\n",
    "        * `balanced` mode: `model = RandomForestClassifier(class_weight='balanced')`\n",
    "            * uses the values of y to automatically adjust weights inversely proportional to class frequencies in the the input data\n",
    "            * this option is available for other classifiers\n",
    "                * `model = LogisticRegression(class_weight='balanced')`\n",
    "                * `model = SVC(kernel='linear', class_weight='balanced', probability=True)`\n",
    "        * `balanced_subsample` mode: `model = RandomForestClassifier(class_weight='balanced_subsample')`\n",
    "            * is the same as the `balanced` option, except weights are calculated again at each iteration of growing a tree in a the random forest\n",
    "            * this option is only applicable for the Random Forest model\n",
    "        * manual input\n",
    "            * adjust weights to any ratio, not just value counts relative to sample\n",
    "            * `class_weight={0:1,1:4}`\n",
    "            * this is a good option to slightly upsample the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "* Random Forest takes many other options to optimize the model\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(n_estimators=10, \n",
    "                               criterion=’gini’, \n",
    "                               max_depth=None, \n",
    "                               min_samples_split=2, \n",
    "                               min_samples_leaf=1, \n",
    "                               max_features=’auto’, \n",
    "                               n_jobs=-1, class_weight=None)\n",
    "```\n",
    "\n",
    "* the shape and size of the trees in a random forest are adjusted with **leaf size** and **tree depth**\n",
    "* `n_estimators`: one of the most important setting is the number of trees in the forest\n",
    "* `max_features`: the number of features considered for splitting at each leaf node\n",
    "* `criterion`: change the way the data is split at each node (default is `gini` coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV for hyperparameter tuning\n",
    "\n",
    "* [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "* `from sklearn.model_selection import GridSearchCV`\n",
    "* `GridSearchCV evaluates all combinations of parameters defined in the parameter grid\n",
    "* Random Forest Parameter Grid:\n",
    "\n",
    "```python\n",
    "# Create the parameter grid \n",
    "param_grid = {'max_depth': [80, 90, 100, 110],\n",
    "              'max_features': [2, 3],\n",
    "              'min_samples_leaf': [3, 4, 5],\n",
    "              'min_samples_split': [8, 10, 12],\n",
    "              'n_estimators': [100, 200, 300, 1000]}\n",
    "\n",
    "# Define which model to use\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_model = GridSearchCV(estimator = model, \n",
    "                                 param_grid = param_grid, \n",
    "                                 cv = 5,\n",
    "                                 n_jobs = -1, \n",
    "                                 scoring='f1')\n",
    "```\n",
    "\n",
    "* define the ML model to be used\n",
    "* put the model into `GridSearchCV`\n",
    "* pass in `param_grid`\n",
    "* frequency of cross-validation\n",
    "* define a scoring metric to evaluate the models\n",
    "    * the default option is accuracy which isn't optimal for fraud detection\n",
    "    * use `precision`, `recall` or `f1`\n",
    "\n",
    "```python\n",
    "# Fit the grid search to the data\n",
    "grid_search_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal parameters \n",
    "grid_search_model.best_params_\n",
    "\n",
    "{'bootstrap': True,\n",
    " 'max_depth': 80,\n",
    " 'max_features': 3,\n",
    " 'min_samples_leaf': 5,\n",
    " 'min_samples_split': 12,\n",
    " 'n_estimators': 100}\n",
    "```\n",
    "\n",
    "* once `GridSearchCV` and `model` are fit to the data, obtain the parameters belonging to the optimal model by using the `best_params_` attribute\n",
    "* `GridSearchCV` is computationally heavy\n",
    "    * Can require many hours, depending on the amount of data and number of parameters in the grid\n",
    "    * __**Save the Results**__\n",
    "\n",
    "```python\n",
    "# Get the best_estimator results\n",
    "grid_search.best_estimator_\n",
    "grid_search.best_score_\n",
    "```\n",
    "\n",
    "* `best_score_`: mean cross-validated score of the `best_estimator_`, which depends on the `scoring` option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model adjustments\n",
    "\n",
    "A simple way to adjust the random forest model to deal with highly imbalanced fraud data, is to use the **`class_weights` option** when defining the `sklearn` model. However, as you will see, it is a bit of a blunt force mechanism and might not work for your very special case.\n",
    "\n",
    "In this exercise you'll explore the ``weight = \"balanced_subsample\"`` mode the Random Forest model from the earlier exercise. You already have split your data in a training and test set, i.e `X_train`, `X_test`, `y_train`, `y_test` are available. The metrics function have already been imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Set the `class_weight` argument of your classifier to `balanced_subsample`.\n",
    "* Fit your model to your training set.\n",
    "* Obtain predictions and probabilities from X_test.\n",
    "* Obtain the `roc_auc_score`, the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with balanced subsample\n",
    "model = RandomForestClassifier(class_weight='balanced_subsample', random_state=5, n_estimators=100)\n",
    "\n",
    "# Fit your training model to your training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain the predicted values and probabilities from the model \n",
    "predicted = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print('ROC Score:')\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, predicted))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that the model results don't improve drastically. We now have 3 less false positives, but now 19 in stead of 18 false negatives, i.e. cases of fraud we are not catching. If we mostly care about catching fraud, and not so much about the false positives, this does actually not improve our model at all, albeit a simple option to try. In the next exercises you'll see how to more smartly tweak your model to focus on reducing false negatives and catch more fraud.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting RF for fraud detection\n",
    "\n",
    "In this exercise you're going to dive into the options for the random forest classifier, as we'll **assign weights** and **tweak the shape** of the decision trees in the forest. You'll **define weights manually**, to be able to off-set that imbalance slightly. In our case we have 300 fraud to 7000 non-fraud cases, so by setting the weight ratio to 1:12, we get to a 1/3 fraud to 2/3 non-fraud ratio, which is good enough for training the model on.\n",
    "\n",
    "The data in this exercise has already been split into training and test set, so you just need to focus on defining your model. You can then use the function `get_model_results()` as a short cut. This function fits the model to your training data, predicts and obtains performance metrics similar to the steps you did in the previous exercises.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Change the `weight` option to set the ratio to 1 to 12 for the non-fraud and fraud cases, and set the split criterion to 'entropy'.\n",
    "* Set the maximum depth to 10.\n",
    "* Set the minimal samples in leaf nodes to 10.\n",
    "* Set the number of trees to use in the model to 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def get_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                      X_test: np.ndarray, y_test: np.ndarray, model):\n",
    "    \"\"\"\n",
    "    model: sklearn model (e.g. RandomForestClassifier)\n",
    "    \"\"\"\n",
    "    # Fit your training model to your training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Obtain the predicted values and probabilities from the model \n",
    "    predicted = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "\n",
    "    # Print the ROC curve, classification report and confusion matrix\n",
    "    print('ROC Score:')\n",
    "    print(roc_auc_score(y_test, probs[:,1]))\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model options\n",
    "model = RandomForestClassifier(bootstrap=True,\n",
    "                               class_weight={0:1, 1:12},\n",
    "                               criterion='entropy',\n",
    "                               # Change depth of model\n",
    "                               max_depth=10,\n",
    "                               # Change the number of samples in leaf nodes\n",
    "                               min_samples_leaf=10, \n",
    "                               # Change the number of trees to use\n",
    "                               n_estimators=20,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=5)\n",
    "\n",
    "# Run the function get_model_results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By smartly defining more options in the model, you can obtain better predictions. You have effectively reduced the number of false negatives, i.e. you are catching more cases of fraud, whilst keeping the number of false positives low. In this exercise you've manually changed the options of the model. There is a smarter way of doing it, by using `GridSearchCV`, which you'll see in the next exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter optimization with GridSearchCV\n",
    "\n",
    "In this exercise you're going to **tweak our model in a less \"random\" way**, but use `GridSearchCV` to do the work for you.\n",
    "\n",
    "With `GridSearchCV` you can define **which performance metric to score** the options on. Since for fraud detection we are mostly interested in catching as many fraud cases as possible, you can optimize your model settings to get the best possible Recall score. If you also cared about reducing the number of false positives, you could optimize on F1-score, this gives you that nice Precision-Recall trade-off.\n",
    "\n",
    "`GridSearchCV` has already been imported from `sklearn.model_selection`, so let's give it a try!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Define in the parameter grid that you want to try 1 and 30 trees, and that you want to try the `gini` and `entropy` split criterion.\n",
    "* Define the model to be simple `RandomForestClassifier`, you want to keep the random_state at 5 to be able to compare models.\n",
    "* Set the `scoring` option such that it optimizes for recall.\n",
    "* Fit the model to the training data `X_train` and `y_train` and obtain the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30],\n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'max_depth': [4, 8],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model results with GridSearchCV\n",
    "\n",
    "You discovered that the **best parameters for your model** are that the split criterion should be set to `'gini'`, the number of estimators (trees) should be 30, the maximum depth of the model should be 8 and the maximum features should be set to `\"log2\"`.\n",
    "\n",
    "Let's give this a try and see how well our model performs. You can use the `get_model_results()` function again to save time.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* Input the optimal settings into the model definition.\n",
    "* Fit the model, obtain predictions and get the performance parameters with `get_model_results()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12},\n",
    "                               criterion='gini',\n",
    "                               max_depth=8,\n",
    "                               max_features='log2', \n",
    "                               min_samples_leaf=10,\n",
    "                               n_estimators=30,\n",
    "                               n_jobs=-1,\n",
    "                               random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model has been improved even further. The number of false positives has now been slightly reduced even further, which means we are catching more cases of fraud. However, you see that the number of false positives actually went up. That is that Precision-Recall trade-off in action. To decide which final model is best, you need to take into account how bad it is not to catch fraudsters, versus how many false positives the fraud analytics team can deal with. Ultimately, this final decision should be made by you and the fraud team together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting weights within the Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection using unlabeled data\n",
    "\n",
    "Use unsupervised learning techniques to detect fraud. Segment customers, use K-means clustering and other clustering algorithms to find suspicious occurrences in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal versus abnormal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using statistics to define normal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods to detect fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning fraud vs. non-fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate clustering methods for fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing smallest clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection using text\n",
    "\n",
    "Use text data, text mining and topic modeling to detect fraudulent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word search with dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using list of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining to detect fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling on fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary and corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagging fraud based on topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding fraudsters based on topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4: Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
